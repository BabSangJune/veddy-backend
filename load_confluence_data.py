# load_confluence_data.py (config í†µí•© + í† í° í•„í„°ë§ ì™„ì„±)

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from services.confluence_service import confluence_service
from services.embedding_service import embedding_service
from services.supabase_service import supabase_service
from services.token_chunk_service import token_chunk_service
from config import CONFLUENCE_URL, VECTOR_SEARCH_CONFIG  # âœ… VECTOR_SEARCH_CONFIG ì¶”ê°€

def load_confluence_documents():
    """Confluence ë¬¸ì„œë¥¼ Supabaseì— ë¡œë“œ (ì™„ì „ config í†µí•©)"""

    print("\n" + "="*60)
    print("ğŸ“š Confluence ë¬¸ì„œ ë¡œë“œ ì‹œì‘ (í† í° ê¸°ë°˜ + config í†µí•©)")
    print(f"ğŸ“Š Config: chunk={VECTOR_SEARCH_CONFIG['chunk_tokens']}tokens, overlap={VECTOR_SEARCH_CONFIG['overlap_tokens']}")
    print("="*60)

    # 1. Confluenceì—ì„œ ë¬¸ì„œ ì¡°íšŒ
    print("\n1ï¸âƒ£ Confluenceì—ì„œ ë¬¸ì„œ ì¡°íšŒ ì¤‘...")
    try:
        pages = confluence_service.get_all_pages_with_content()
    except Exception as e:
        print(f"âŒ Confluence ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return

    if not pages:
        print("âŒ Confluenceì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nâœ… {len(pages)}ê°œ í˜ì´ì§€ ì¡°íšŒ ì™„ë£Œ")

    # 2. ê° í˜ì´ì§€ë³„ë¡œ ì²­í¬ ë¶„í•  ë° ì €ì¥
    print("\n2ï¸âƒ£ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    success_count = 0
    error_count = 0

    for idx, page in enumerate(pages, 1):
        try:
            page_title = page.get('title', 'ì œëª© ì—†ìŒ')
            page_id = page.get('page_id', '')
            page_content = page.get('content', '')
            page_url = page.get('url', '')
            page_labels = page.get('labels', [])

            print(f"\n [{idx}/{len(pages)}] í˜ì´ì§€ ì²˜ë¦¬: {page_title}")

            # âœ… í† í° ê¸°ë°˜ í•„í„°ë§ (config í†µí•©)
            text_stats = token_chunk_service.get_text_stats(page_content)
            print(f"   ğŸ“Š ì›ë³¸: {text_stats['char_count']}ì / {text_stats['token_count']}í† í°")

            if text_stats['token_count'] < VECTOR_SEARCH_CONFIG['min_chunk_tokens']:  # âœ… í† í° ê¸°ì¤€ ë³€ê²½
                print(f"   âš ï¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ì„œ ìŠ¤í‚µ (í† í°: {text_stats['token_count']})")
                continue

            # ===== URL ì²˜ë¦¬ =====
            full_url = page_url
            if page_url and not page_url.startswith('http'):
                full_url = f"{CONFLUENCE_URL}{page_url}"

            print(f"   ğŸ”— URL: {full_url}")

            # 1. ë¬¸ì„œ ì €ì¥
            print(f"   â”œâ”€ ë¬¸ì„œ ì €ì¥ ì¤‘...")
            saved_doc = supabase_service.add_document(
                source="confluence",
                source_id=page_id,
                title=page_title,
                content=page_content,
                metadata={
                    'url': full_url,
                    'page_url': full_url,
                    'labels': page_labels,
                    'source': 'confluence',
                    'confluence_id': page_id,
                    'token_count': text_stats['token_count']
                }
            )

            document_id = saved_doc.get("id")
            if not document_id:
                print(f"   âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨")
                error_count += 1
                continue

            print(f"   â”œâ”€ âœ… ë¬¸ì„œ ì €ì¥ ì™„ë£Œ (ID: {document_id})")

            # 2. âœ… config ê¸°ë°˜ í† í° ì²­í‚¹
            print(f"   â”œâ”€ í† í° ê¸°ë°˜ ì²­í¬ ë¶„í•  ì¤‘...")
            chunks = token_chunk_service.chunk_text(
                page_content,
                chunk_tokens=VECTOR_SEARCH_CONFIG['chunk_tokens'],      # âœ… config ì‚¬ìš©
                overlap_tokens=VECTOR_SEARCH_CONFIG['overlap_tokens'],  # âœ… config ì‚¬ìš©
                min_chunk_tokens=VECTOR_SEARCH_CONFIG['min_chunk_tokens']  # âœ… config ì‚¬ìš©
            )
            print(f"   â”œâ”€ âœ… {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  (í† í° ê¸°ë°˜)")

            # 3. ì„ë² ë”© & ì €ì¥
            print(f"   â”œâ”€ ë²¡í„° ì„ë² ë”© ì¤‘...")
            embeddings = embedding_service.embed_batch(chunks)

            for chunk_num, (chunk_content, embedding) in enumerate(zip(chunks, embeddings), 1):
                chunk_stats = token_chunk_service.get_text_stats(chunk_content)
                supabase_service.add_chunk(
                    document_id=document_id,
                    chunk_number=chunk_num,
                    content=chunk_content,
                    embedding=embedding
                )
                print(f"   â”‚  â””â”€ ì²­í¬ {chunk_num}: {chunk_stats['char_count']}ì / {chunk_stats['token_count']}í† í°")

            print(f"   â””â”€ âœ… {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
            success_count += 1

        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            error_count += 1
            continue

    # 3. ìµœì¢… í†µê³„
    print("\n" + "="*60)
    print("âœ… Confluence ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ!")
    print("="*60)

    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   - ì„±ê³µ: {success_count}ê°œ")
    print(f"   - ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"   - ì „ì²´: {len(pages)}ê°œ")

    # Supabase í†µê³„
    try:
        all_docs = supabase_service.list_documents(limit=100)
        confluence_docs = [d for d in all_docs if d.get("source") == "confluence"]
        print(f"\nğŸ“Š Supabase í†µê³„:")
        print(f"   - ì „ì²´ ë¬¸ì„œ: {len(all_docs)}ê°œ")
        print(f"   - Confluence ë¬¸ì„œ: {len(confluence_docs)}ê°œ")
    except Exception as e:
        print(f"\nâš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    load_confluence_documents()
