# backend/routers/chat_router.py
# âœ… ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í†µí•© ì™„ë£Œ

from fastapi import APIRouter, Depends, Request
from fastapi.responses import StreamingResponse
from typing import AsyncGenerator
from model.schemas import ChatRequest
from services.langchain_rag_service import langchain_rag_service
from services.supabase_service import SupabaseService
from services.conversation_service import ConversationService  # ğŸ†• ì¶”ê°€
from auth.auth_service import verify_supabase_token
from auth.user_service import user_service
import asyncio
import re
import json
from datetime import datetime

from logging_config import get_logger, generate_request_id
import logging

base_logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/chat", tags=["chat"])

@router.post("/stream")
async def chat_stream(
        request_body: ChatRequest,
        request: Request,
        user: dict = Depends(verify_supabase_token)
):
    user_id = user["user_id"]
    email = user.get("email")
    name = user.get("name")
    access_token = user["access_token"]

    request_id = generate_request_id()
    logger = get_logger(__name__, user_id=user_id, request_id=request_id, email=email)

    logger.info("ì±„íŒ… ìš”ì²­ ìˆ˜ì‹ ", extra={
        "query_length": len(request_body.query),
        "table_mode": request_body.table_mode
    })

    user_fk = await user_service.get_or_create_user(
        user_id=user_id,
        email=email,
        name=name,
        auth_type="general"
    )
    logger.info("ì‚¬ìš©ì ì •ë³´ í™•ì¸", extra={"user_fk": user_fk})

    user_supabase = SupabaseService(access_token=access_token)

    async def generate_stream() -> AsyncGenerator[str, None]:
        full_response = ""
        source_chunk_ids = []
        conversation_id = None  # ğŸ†•

        try:
            logger.info("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘", extra={
                "query_preview": request_body.query[:50]
            })

            async def rag_with_timeout():
                nonlocal full_response, source_chunk_ids, conversation_id

                # ğŸ†• Step 1: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
                conversation_service = ConversationService(user_supabase)

                conversation_id = conversation_service.get_or_create_conversation(
                    user_id=user_id,
                    user_fk=user_fk,
                    conversation_id=getattr(request_body, 'conversation_id', None)
                )

                history = conversation_service.get_conversation_history(
                    conversation_id=conversation_id,
                    limit=10
                )

                history_text = conversation_service.format_history_for_prompt(
                    history=history,
                    max_turns=5
                )

                logger.info("ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ", extra={
                    "conversation_id": conversation_id,
                    "history_turns": len(history) // 2 if history else 0
                })

                # ğŸ†• Step 2: ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì¿¼ë¦¬ ìƒì„±
                if history_text:
                    contextual_query = f'''ì´ì „ ëŒ€í™”:
                    {history_text}
                    
                    í˜„ì¬ ì§ˆë¬¸: {request_body.query}
                    
                    ìœ„ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.'''
                else:
                    contextual_query = request_body.query

                # Step 3: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                from services.embedding_service import embedding_service
                from services.langchain_rag_service import SupabaseRetriever, CustomEmbeddings

                logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ (PGroonga + pgvector)")

                embeddings = CustomEmbeddings()
                retriever = SupabaseRetriever(
                    embeddings=embeddings,
                    supabase_client=user_supabase,
                    k=5,
                    threshold=0.3
                )

                _, raw_chunks = retriever.search_hybrid(request_body.query)
                source_chunk_ids = [chunk.get('id') for chunk in raw_chunks if chunk.get('id')]

                logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ", extra={
                    "chunks_found": len(source_chunk_ids),
                    "search_mode": "PGroonga + pgvector + Reranking"
                })

                # Step 4: LLM ì‘ë‹µ ìƒì„±
                logger.info("LLM ì‘ë‹µ ìƒì„± ì‹œì‘")

                for token in langchain_rag_service.process_query_streaming(
                        user_id=user_id,
                        query=contextual_query,  # ğŸ†• ì»¨í…ìŠ¤íŠ¸ í¬í•¨
                        table_mode=request_body.table_mode,
                        supabase_client=user_supabase
                ):
                    if await request.is_disconnected():
                        logger.warning("í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€")
                        raise asyncio.CancelledError("Client disconnected")

                    if token:
                        full_response += token

                logger.info("LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ", extra={
                    "response_length": len(full_response)
                })

            try:
                await asyncio.wait_for(rag_with_timeout(), timeout=120.0)
            except asyncio.TimeoutError:
                logger.error("RAG ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ", extra={"timeout_seconds": 120})
                yield f" {json.dumps({'type': 'error', 'error': 'ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
                return

            # Step 5: í¬ë§·íŒ…
            formatted = re.sub(r'(\d+\.)\s+', r'\1\n\n', full_response)
            formatted = re.sub(r'(#{1,3})\s+([^\n]+)', r'\1 \2\n\n', formatted)
            formatted = re.sub(r'(-\s+[^\n]+)', r'\1\n', formatted)

            if 'ì°¸ê³  ë¬¸ì„œ' not in formatted:
                formatted += '\n\nğŸ“š ì°¸ê³  ë¬¸ì„œ:\n'

            formatted = re.sub(r'\n{4,}', '\n\n', formatted)

            # Step 6: ë©”ì‹œì§€ ì €ì¥
            try:
                user_supabase.client.table("messages").insert({
                    "user_id": user_id,
                    "user_fk": user_fk,
                    "user_query": request_body.query,
                    "ai_response": formatted,
                    "conversation_id": conversation_id,  # ğŸ†•
                    "source_chunk_ids": source_chunk_ids if source_chunk_ids else None,
                    "usage": {},
                    "created_at": datetime.utcnow().isoformat()
                }).execute()

                # ğŸ†• ì²« ë©”ì‹œì§€ë©´ ì œëª© ì—…ë°ì´íŠ¸
                if conversation_id:
                    conversation_service = ConversationService(user_supabase)
                    history = conversation_service.get_conversation_history(
                        conversation_id=conversation_id,
                        limit=2
                    )

                    if len(history) <= 1:
                        title = request_body.query[:50] + "..." if len(request_body.query) > 50 else request_body.query
                        conversation_service.update_conversation_title(
                            conversation_id=conversation_id,
                            title=title
                        )

                logger.info("ë©”ì‹œì§€ ì €ì¥ ì™„ë£Œ", extra={
                    "chunks_count": len(source_chunk_ids),
                    "conversation_id": conversation_id
                })
            except Exception as save_error:
                logger.error("ë©”ì‹œì§€ ì €ì¥ ì‹¤íŒ¨", extra={"error": str(save_error)})

            # Step 7: ì „ì†¡
            logger.info("í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡ ì‹œì‘")

            for i, char in enumerate(formatted):
                if i % 100 == 0 and await request.is_disconnected():
                    logger.warning("í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€", extra={"sent_chars": i})
                    return

                data = json.dumps({"token": char, "type": "token"}, ensure_ascii=False)
                output = f" {data}\n\n"
                yield output
                await asyncio.sleep(0.001)

            yield f" {json.dumps({'type': 'done'}, ensure_ascii=False)}\n\n"

            logger.info("ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ", extra={
                "total_chars": len(formatted),
                "status": "success"
            })

        except asyncio.CancelledError:
            logger.warning("ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œë¨")
            return

        except Exception as e:
            logger.error("ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜", extra={
                "error": str(e),
                "error_type": type(e).__name__
            }, exc_info=True)

            error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            if "timeout" in str(e).lower():
                error_msg = "ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
            elif "connection" in str(e).lower():
                error_msg = "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            error_msg += " ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

            try:
                yield f" {json.dumps({'type': 'error', 'error': error_msg}, ensure_ascii=False)}\n\n"
            except:
                logger.error("ì—ëŸ¬ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream; charset=utf-8",
            "X-Request-ID": request_id
        }
    )
