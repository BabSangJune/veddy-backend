# routers/admin_router.py

from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, AsyncGenerator
import asyncio
import json
import time

from services.confluence_service import ConfluenceService
from services.supabase_service import supabase_service
from services.embedding_service import embedding_service
from services.token_chunk_service import token_chunk_service
from auth.auth_service import verify_supabase_token
from logging_config import get_logger

router = APIRouter(prefix="/api/admin", tags=["admin"])

class LoadConfluenceDataRequest(BaseModel):
    """Confluence ë°ì´í„° ë¡œë“œ ìš”ì²­"""
    space_key: str
    atlassian_id: str
    api_token: str


# ===== 1ï¸âƒ£ POST ì—”ë“œí¬ì¸íŠ¸ (ì´ˆê¸° ìš”ì²­) =====

@router.post("/confluence/load")
async def load_confluence_data(
        request: LoadConfluenceDataRequest,
        user: dict = Depends(verify_supabase_token)
):
    """
    âœ¨ Confluence ë°ì´í„° ë¡œë“œ ìš”ì²­ (ì´ˆê¸°)

    ì¦‰ì‹œ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ ì‘ë‹µì˜ stream_endpointë¥¼ ì‚¬ìš©í•´ SSE ìŠ¤íŠ¸ë¦¼ì— ì—°ê²°í•©ë‹ˆë‹¤.
    """
    logger = get_logger(__name__, user_id=user["user_id"])

    logger.info(
        "ğŸ“š Confluence ë°ì´í„° ë¡œë“œ ìš”ì²­ (POST)",
        extra={
            "space_key": request.space_key,
            "atlassian_id": request.atlassian_id[:20] + "***"
        }
    )

    try:
        # âœ… ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
        return {
            "status": "accepted",
            "message": f"âœ… Space '{request.space_key}'ì˜ ë°ì´í„° ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
            "space_key": request.space_key,
            "stream_endpoint": f"/api/admin/confluence/load-stream"
        }

    except Exception as e:
        logger.error(f"âŒ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        )


# ===== 2ï¸âƒ£ GET SSE ì—”ë“œí¬ì¸íŠ¸ (ì§„í–‰ ìƒí™©) =====

# routers/admin_router.py

@router.get("/confluence/load-stream")
async def load_confluence_data_stream(
        space_key: str,
        atlassian_id: str,
        api_token: str,
        user: dict = Depends(verify_supabase_token)
):
    """
    âœ¨ SSE: Confluence ë°ì´í„° ë¡œë“œ (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©)
    Query Parameter í˜•ì‹ìœ¼ë¡œ ìê²©ì¦ëª… ì „ë‹¬
    """
    logger = get_logger(__name__, user_id=user["user_id"])

    async def event_generator():
        """SSE ì´ë²¤íŠ¸ ìƒì„±ê¸°"""
        try:
            # 1ï¸âƒ£ ì‹œì‘ ì´ë²¤íŠ¸
            yield f"data: {json.dumps({'status': 'started', 'message': 'Confluence ë°ì´í„° ë¡œë“œ ì‹œì‘'})}\n\n"

            confluence_service = ConfluenceService.initialize(
                space_key=space_key,
                atlassian_id=atlassian_id,
                api_token=api_token
            )

            pages = confluence_service.get_all_pages_with_content()

            if not pages:
                yield f"data: {json.dumps({'status': 'error', 'message': f'{space_key}ì—ì„œ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ'})}\n\n"
                return

            total_pages = len(pages)
            success_count = 0
            skip_count = 0
            error_count = 0
            total_chunks = 0

            # 2ï¸âƒ£ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ì•Œë¦¼
            yield f"data: {json.dumps({
                'status': 'pages_loaded',
                'total_pages': total_pages,
                'message': f'ì´ {total_pages}ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ. ì²˜ë¦¬ ì‹œì‘í•©ë‹ˆë‹¤.',
                'progress_percent': 5
            })}\n\n"

            # 3ï¸âƒ£ ê° í˜ì´ì§€ ì²˜ë¦¬
            for idx, page in enumerate(pages, 1):
                try:
                    page_title = page.get('title', 'ì œëª© ì—†ìŒ')
                    page_id = page.get('page_id', '')
                    page_content = page.get('content', '')
                    page_url = page.get('url', '')
                    page_labels = page.get('labels', [])
                    created_at = page.get('created_at')
                    updated_at = page.get('updated_at')
                    version_number = page.get('version_number', 1)

                    # ì§„í–‰ ìƒí™© ì•Œë¦¼ (ì²˜ë¦¬ ì‹œì‘)
                    progress = int(5 + ((idx - 1) / total_pages) * 90)  # 5% ~ 95%
                    yield f"data: {json.dumps({
                        'status': 'processing',
                        'message': f'[{idx}/{total_pages}] {page_title} ì²˜ë¦¬ ì¤‘...',
                        'current_page': page_title,
                        'processed_pages': idx,
                        'total_pages': total_pages,
                        'progress_percent': progress,
                        'success_count': success_count,
                        'skip_count': skip_count,
                        'error_count': error_count,
                        'total_chunks': total_chunks
                    })}\n\n"

                    # ê¸°ì¡´ ë¬¸ì„œ í™•ì¸
                    existing_doc = supabase_service.get_document_by_source_id(
                        source="confluence",
                        source_id=page_id
                    )

                    # updated_at ë¹„êµí•´ì„œ ë³€ê²½ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    if existing_doc:
                        existing_updated_at = existing_doc.get("updated_at")
                        confluence_updated_str = updated_at.isoformat() if updated_at else ""
                        if existing_updated_at == confluence_updated_str:
                            skip_count += 1
                            continue

                    # í† í° í•„í„°ë§
                    text_stats = token_chunk_service.get_text_stats(page_content)
                    if text_stats['token_count'] < 30:
                        skip_count += 1
                        continue

                    # ë¬¸ì„œ ì €ì¥
                    saved_doc = supabase_service.add_document(
                        source="confluence",
                        source_id=page_id,
                        title=page_title,
                        content=page_content,
                        metadata={
                            'url': page_url,
                            'page_url': page_url,
                            'labels': page_labels,
                            'source': 'confluence',
                            'confluence_id': page_id,
                            'token_count': text_stats['token_count'],
                            'space_key': space_key,
                            'version_number': version_number
                        },
                        created_at=created_at,
                        updated_at=updated_at
                    )

                    document_id = saved_doc.get("id")
                    if not document_id:
                        error_count += 1
                        continue

                    # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
                    if existing_doc:
                        supabase_service.delete_chunks_by_document_id(document_id)

                    # ì²­í¬ ë¶„í• 
                    chunks = token_chunk_service.chunk_text(
                        page_content,
                        chunk_tokens=400,
                        overlap_tokens=50,
                        min_chunk_tokens=30
                    )

                    # âœ… ì„ë² ë”© ì „ ì§„í–‰ ìƒí™© ì•Œë¦¼
                    yield f"data: {json.dumps({
                        'status': 'embedding',
                        'message': f'[{idx}/{total_pages}] {page_title} ì„ë² ë”© ì¤‘... ({len(chunks)}ê°œ ì²­í¬)',
                        'current_page': page_title,
                        'processed_pages': idx,
                        'total_pages': total_pages,
                        'progress_percent': progress,
                        'success_count': success_count,
                        'skip_count': skip_count,
                        'error_count': error_count,
                        'total_chunks': total_chunks
                    })}\n\n"

                    # ë²¡í„° ì„ë² ë”©
                    embeddings = embedding_service.embed_batch(chunks)

                    # âœ… ì„ë² ë”© í›„ ì²­í¬ ì €ì¥ ì§„í–‰ ìƒí™©
                    for chunk_num, (chunk_content, embedding) in enumerate(zip(chunks, embeddings), 1):
                        supabase_service.add_chunk(
                            document_id=document_id,
                            chunk_number=chunk_num,
                            content=chunk_content,
                            embedding=embedding
                        )
                        total_chunks += 1

                    success_count += 1

                    # âœ… í˜ì´ì§€ ì™„ë£Œ ì•Œë¦¼
                    yield f"data: {json.dumps({
                        'status': 'page_completed',
                        'message': f'[{idx}/{total_pages}] {page_title} ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)',
                        'current_page': page_title,
                        'processed_pages': idx,
                        'total_pages': total_pages,
                        'progress_percent': progress,
                        'success_count': success_count,
                        'skip_count': skip_count,
                        'error_count': error_count,
                        'total_chunks': total_chunks
                    })}\n\n"

                except Exception as e:
                    logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
                    error_count += 1
                    yield f"data: {json.dumps({
                        'status': 'page_error',
                        'message': f'âŒ í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
                        'processed_pages': idx,
                        'total_pages': total_pages,
                        'error_count': error_count
                    })}\n\n"
                    continue

            # âœ… ìµœì¢… ì™„ë£Œ
            yield f"data: {json.dumps({
                'status': 'completed',
                'success_count': success_count,
                'skip_count': skip_count,
                'error_count': error_count,
                'total_chunks': total_chunks,
                'progress_percent': 100,
                'message': f'âœ… {success_count}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ ({total_chunks}ê°œ ì²­í¬ ìƒì„±)'
            })}\n\n"

            logger.info(
                "âœ… Confluence ë°ì´í„° ë¡œë“œ ì™„ë£Œ",
                extra={
                    "space_key": space_key,
                    "success": success_count,
                    "skip": skip_count,
                    "error": error_count,
                    "chunks": total_chunks
                }
            )

        except Exception as e:
            logger.error(f"SSE ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}", exc_info=True)
            yield f"data: {json.dumps({'status': 'error', 'message': f'ì˜¤ë¥˜: {str(e)}'})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")



# ===== 3ï¸âƒ£ GET ìƒíƒœ ì¡°íšŒ =====

@router.get("/confluence/status")
async def get_confluence_status(
        user: dict = Depends(verify_supabase_token)
):
    """
    âœ¨ ê´€ë¦¬ì: í˜„ì¬ Confluence ìƒíƒœ í™•ì¸
    """
    logger = get_logger(__name__, user_id=user["user_id"])

    try:
        all_docs = supabase_service.list_documents(limit=1000)
        confluence_docs = [d for d in all_docs if d.get("source") == "confluence"]

        space_stats = {}
        for doc in confluence_docs:
            space_key = doc.get("metadata", {}).get("space_key", "unknown")
            if space_key not in space_stats:
                space_stats[space_key] = {"count": 0, "docs": []}
            space_stats[space_key]["count"] += 1
            space_stats[space_key]["docs"].append({
                "id": doc.get("id"),
                "title": doc.get("title")
            })

        logger.info(
            "Confluence ìƒíƒœ ì¡°íšŒ",
            extra={
                "total_docs": len(confluence_docs),
                "space_count": len(space_stats)
            }
        )

        return {
            "status": "success",
            "total_documents": len(confluence_docs),
            "total_spaces": len(space_stats),
            "space_stats": space_stats
        }

    except Exception as e:
        logger.error(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/confluence/progress")
async def get_confluence_progress(
        user: dict = Depends(verify_supabase_token)
):
    """
    âœ¨ í´ë§ìš©: í˜„ì¬ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì¡°íšŒ
    """
    return {
        "status": "processing",
        "message": "ì§„í–‰ ì¤‘..."
    }
