# services/token_chunk_service.py
from transformers import AutoTokenizer
from typing import List
import logging

logger = logging.getLogger(__name__)

class TokenChunkService:
    """
    dragonkue/BGE-m3-ko í† í¬ë‚˜ì´ì € ê¸°ë°˜ í† í° ì²­í‚¹ ì„œë¹„ìŠ¤
    """

    def __init__(self, model_name: str = "dragonkue/BGE-m3-ko"):
        """
        ì´ˆê¸°í™”: í† í¬ë‚˜ì´ì € ë¡œë“œ (í•œ ë²ˆë§Œ)
        """
        logger.info(f"ğŸ”§ TokenChunkService ì´ˆê¸°í™” ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model_name = model_name
        logger.info("âœ… TokenChunkService ì´ˆê¸°í™” ì™„ë£Œ")

    def chunk_text(
            self,
            text: str,
            chunk_tokens: int = 400,
            overlap_tokens: int = 50,
            min_chunk_tokens: int = 50
    ) -> List[str]:
        """
        í† í° ê¸°ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            chunk_tokens: ì²­í¬ë‹¹ í† í° ìˆ˜ (ê¶Œì¥: 300-512)
            overlap_tokens: ì˜¤ë²„ë© í† í° ìˆ˜ (ê¶Œì¥: 50-100)
            min_chunk_tokens: ìµœì†Œ í† í° ìˆ˜ (ë„ˆë¬´ ì§§ì€ ì²­í¬ í•„í„°ë§)

        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not text or not text.strip():
            return []

        logger.debug(f"ğŸ“„ ì²­í‚¹ ì‹œì‘: {len(text)}ì, target={chunk_tokens}tokens")

        # 1. í† í°í™” (íŠ¹ìˆ˜ í† í° ì œì™¸)
        tokens = self.tokenizer.encode(
            text,
            add_special_tokens=False,
            truncation=False
        )

        logger.debug(f"ğŸ”¢ í† í°í™” ì™„ë£Œ: {len(tokens)} tokens")

        # 2. ì²­í¬ ìƒì„±
        chunks = []
        start = 0

        while start < len(tokens):
            # ì²­í¬ í† í° ë²”ìœ„
            end = min(start + chunk_tokens, len(tokens))
            chunk_token_ids = tokens[start:end]

            # í† í° â†’ í…ìŠ¤íŠ¸ ë””ì½”ë”©
            chunk_text = self.tokenizer.decode(
                chunk_token_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )

            # ìµœì†Œ í† í° ìˆ˜ ì²´í¬
            chunk_token_len = len(chunk_token_ids)
            if chunk_token_len >= min_chunk_tokens and chunk_text.strip():
                chunks.append(chunk_text.strip())
                logger.debug(f"âœ… ì²­í¬ ìƒì„±: {chunk_token_len}tokens")

            # ë‹¤ìŒ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
            start = end - overlap_tokens

            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if end >= len(tokens):
                break

        logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        return chunks

    def get_text_stats(self, text: str) -> dict:
        """í…ìŠ¤íŠ¸ í†µê³„ (ë””ë²„ê¹…ìš©)"""
        tokens = self.tokenizer.encode(text, add_special_tokens=False)
        return {
            'char_count': len(text),
            'token_count': len(tokens),
            'avg_tokens_per_char': len(tokens) / len(text) if text else 0
        }

# ê¸€ë¡œë²Œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
token_chunk_service = TokenChunkService()
