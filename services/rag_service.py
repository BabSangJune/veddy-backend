from services.embedding_service import embedding_service
from services.supabase_service import supabase_service
from typing import List, Dict, Any, Tuple
from openai import OpenAI
from config import OPENAI_API_KEY


class RAGService:
    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.embedding_service = embedding_service
        self.supabase_service = supabase_service
        self.openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print("âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def search_relevant_chunks(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        """
        # 1. ì§ˆë¬¸ ë²¡í„°í™”
        query_embedding = self.embedding_service.embed_text(query)

        # 2. ëª¨ë“  ì²­í¬ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ì—†ìŒ!)
        try:
            response = self.supabase_service.client.table("document_chunks") \
                .select("*") \
                .execute()

            chunks_list = response.data
            print(f"ğŸ“š ì´ {len(chunks_list)}ê°œ ì²­í¬ ë¡œë“œë¨ (1íšŒ)")

        except Exception as e:
            print(f"âŒ ì²­í¬ ë¡œë“œ ì˜¤ë¥˜: {e}")
            chunks_list = []

        # 3. ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°
        chunks_with_similarity = []
        for chunk in chunks_list:
            similarity = self._cosine_similarity(query_embedding, chunk.get("embedding", []))
            chunks_with_similarity.append({
                **chunk,
                "similarity": similarity
            })

        # ìƒìœ„ top_kê°œ ë°˜í™˜
        sorted_chunks = sorted(chunks_with_similarity, key=lambda x: x["similarity"], reverse=True)

        print(f"ğŸ” ìƒìœ„ {top_k}ê°œ ì²­í¬:")
        for i, chunk in enumerate(sorted_chunks[:top_k], 1):
            print(f"   {i}. ìœ ì‚¬ë„: {chunk['similarity']:.4f}")

        return sorted_chunks[:top_k]


    def _cosine_similarity(self, vec1, vec2) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np
        import json

        # None ì²´í¬
        if vec1 is None or vec2 is None:
            return 0.0

        try:
            # ë¬¸ìì—´ì´ë©´ íŒŒì‹±
            if isinstance(vec1, str):
                # ì–‘ìª½ ëŒ€ê´„í˜¸ ì œê±° í›„ ì‰¼í‘œë¡œ ë¶„ë¦¬
                vec1_str = vec1.strip('[]').strip()
                try:
                    # ë¨¼ì € JSON ì‹œë„
                    vec1 = json.loads(vec1)
                except:
                    # JSON ì‹¤íŒ¨í•˜ë©´ ì‰¼í‘œë¡œ ë¶„ë¦¬
                    vec1 = [float(x.strip()) for x in vec1_str.split(',') if x.strip()]

            if isinstance(vec2, str):
                # ì–‘ìª½ ëŒ€ê´„í˜¸ ì œê±° í›„ ì‰¼í‘œë¡œ ë¶„ë¦¬
                vec2_str = vec2.strip('[]').strip()
                try:
                    # ë¨¼ì € JSON ì‹œë„
                    vec2 = json.loads(vec2)
                except:
                    # JSON ì‹¤íŒ¨í•˜ë©´ ì‰¼í‘œë¡œ ë¶„ë¦¬
                    vec2 = [float(x.strip()) for x in vec2_str.split(',') if x.strip()]

            # numpy arrayë¡œ ë³€í™˜
            v1 = np.array(vec1, dtype=np.float32)
            v2 = np.array(vec2, dtype=np.float32)

            # í¬ê¸° í™•ì¸
            if v1.size == 0 or v2.size == 0:
                return 0.0

            if v1.shape[0] != v2.shape[0]:
                print(f"âš ï¸ ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: {v1.shape[0]} vs {v2.shape[0]}")
                return 0.0

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = float(dot_product / (norm1 * norm2))
            return similarity

        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return 0.0



    def generate_answer(self, query: str, context_chunks: List[Dict[str, Any]]) -> Tuple[str, Dict[str, int]]:
        """
        LLMì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_chunks: ê²€ìƒ‰ëœ ê´€ë ¨ ì²­í¬ë“¤

        Returns:
            (ë‹µë³€, í† í° ì‚¬ìš©ëŸ‰)
        """
        # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
        context_text = "\n---\n".join([
            f"ì¶œì²˜: {chunk.get('content', '')} (ì‹ ë¢°ë„: {chunk.get('similarity', 0):.2%})"
            for chunk in context_chunks
        ])

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.
ë„ˆëŠ” ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•œ ì±—ë´‡ì´ì•¼.

ë‹¤ìŒ ê·œì¹™ì„ ì§€ì¼œ:
1. ì œê³µëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•´
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´
3. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´
4. ì¶œì²˜ë¥¼ ëª…ì‹œí•´
"""

        user_message = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {query}
"""

        # GPT í˜¸ì¶œ
        response = self.openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=1000
        )

        answer = response.choices[0].message.content
        usage = {
            "input_tokens": response.usage.prompt_tokens,
            "output_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens
        }

        return answer, usage

    def process_query(self, user_id: str, query: str) -> Dict[str, Any]:
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        1. ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
        2. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        3. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        4. ë©”ì‹œì§€ ì €ì¥
        """
        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        relevant_chunks = self.search_relevant_chunks(query, top_k=5)

        # 2. ë‹µë³€ ìƒì„±
        answer, usage = self.generate_answer(query, relevant_chunks)

        # 3. ë©”ì‹œì§€ ì €ì¥
        source_chunk_ids = [chunk.get("id") for chunk in relevant_chunks]
        self.supabase_service.save_message(
            user_id=user_id,
            user_query=query,
            ai_response=answer,
            source_chunk_ids=source_chunk_ids,
            usage=usage
        )

        return {
            "user_query": query,
            "ai_response": answer,
            "source_chunks": relevant_chunks,
            "usage": usage
        }

    def process_query_streaming(self, user_id: str, query: str):
        """
        ìŠ¤íŠ¸ë¦¬ë° RAG ë‹µë³€ ìƒì„± (ì¼ë°˜ ì¿¼ë¦¬ì™€ ë™ì¼í•œ ë°©ì‹)

        yieldë¡œ í† í°ì„ í•˜ë‚˜ì”© ë°˜í™˜
        """

        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (top_k=5ë¡œ í†µì¼)
        relevant_chunks = self.search_relevant_chunks(query, top_k=5)

        # 2. ì»¨í…ìŠ¤íŠ¸ ì¡°í•© (generate_answerì™€ ë™ì¼)
        context_text = "\n---\n".join([
            f"ì¶œì²˜: {chunk.get('content', '')[:100]}... (ì‹ ë¢°ë„: {chunk.get('similarity', 0):.2%})"
            for chunk in relevant_chunks
        ])

        # 3. í”„ë¡¬í”„íŠ¸ (generate_answerì™€ ë™ì¼)
        system_prompt = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.
    ë„ˆëŠ” ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•œ ì±—ë´‡ì´ì•¼.
    
    ë‹¤ìŒ ê·œì¹™ì„ ì§€ì¼œ:
    1. ì œê³µëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•´
    2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´
    3. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´
    4. ì¶œì²˜ë¥¼ ëª…ì‹œí•´
    """

        user_message = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ë¬¸ì„œ:
    {context_text}
    
    ì§ˆë¬¸: {query}
    """

        # 4. OpenAI ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        with self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.7,
                max_tokens=1000,
                stream=True
        ) as stream:
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
rag_service = RAGService()
