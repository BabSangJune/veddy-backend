from sentence_transformers import SentenceTransformer
from typing import List
import numpy as np
import time
from config import EMBEDDING_MODEL_NAME, EMBEDDING_MODEL_DIMENSION, EMBEDDING_BATCH_SIZE


class EmbeddingService:
    def __init__(self):
        """BGE-m3-ko ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“š Embedding ëª¨ë¸ ë¡œë“œ ì¤‘: {EMBEDDING_MODEL_NAME}")
        self.model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print("âœ… Embedding ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def embed_text(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        embedding = self.model.encode(text, convert_to_tensor=False)
        return embedding.astype(np.float32).tolist()

    def embed_batch(self, texts: List[str], batch_size: int = None) -> List[List[float]]:
        """
        ë°°ì¹˜ ì„ë² ë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  - 2000+ ë¬¸ì„œ ì§€ì›)

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ê°œìˆ˜ (ê¸°ë³¸: EMBEDDING_BATCH_SIZE=32)

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []

        batch_size = batch_size or EMBEDDING_BATCH_SIZE
        all_embeddings = []

        print(f"ğŸ”¤ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸ | ë°°ì¹˜í¬ê¸°={batch_size}")
        start_time = time.time()

        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            try:
                embeddings = self.model.encode(batch, convert_to_tensor=False)
                all_embeddings.extend([emb.astype(np.float32).tolist() for emb in embeddings])

                progress = min(i + batch_size, len(texts))
                elapsed = time.time() - start_time
                progress_percent = (progress / len(texts)) * 100
                print(f"  âœ… ì§„í–‰: {progress}/{len(texts)} ({progress_percent:.1f}%) | {elapsed:.2f}ì´ˆ")

            except Exception as e:
                print(f"  âŒ ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}-{i+len(batch)}): {e}")
                raise

        total_time = time.time() - start_time
        print(f"âœ… ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(all_embeddings)}ê°œ | ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")

        return all_embeddings


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
embedding_service = EmbeddingService()
