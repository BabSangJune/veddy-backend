
import re
import logging
from unicodedata import normalize as unicode_normalize
from typing import List, Dict, Any, Generator, Optional
from datetime import datetime

# LangChain 1.0 Import
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool
from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service, SupabaseService
from config import OPENAI_API_KEY, VECTOR_SEARCH_CONFIG, RERANKER_CONFIG

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====

class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)

# ===== Supabase Retriever (config í†µí•©) =====

class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼ (URL ì™„ë²½ ë³´ì¡´, config ê¸°ë°˜)"""

    def __init__(self, embeddings: Embeddings, supabase_client: SupabaseService,
                 k: int = 5, threshold: float = None, ef_search: int = None):
        self.embeddings = embeddings
        self.supabase_client = supabase_client

        # âœ… configì—ì„œ ê¸°ë³¸ê°’ ìë™ ì ìš©
        self.k = k
        self.threshold = threshold or VECTOR_SEARCH_CONFIG['similarity_threshold']
        self.ef_search = ef_search or VECTOR_SEARCH_CONFIG['ef_search']

        logger.info(f"Retriever ì´ˆê¸°í™” | k={self.k} | threshold={self.threshold} | ef_search={self.ef_search}")

    def _get_chunk_url(self, chunk: Dict) -> str:
        """âœ… ì²­í¬ì—ì„œ URL ì¶”ì¶œ (3ê°€ì§€ ë°©ë²• ì‹œë„)

        1. chunkì— url í•„ë“œê°€ ì§ì ‘ ìˆìœ¼ë©´ ì‚¬ìš©
        2. chunkì˜ metadataì—ì„œ íŒŒì‹±
        3. document_idë¡œ documents í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        """

        # 1. chunkì— url í•„ë“œê°€ ì§ì ‘ ìˆìœ¼ë©´
        if chunk.get('url') and chunk.get('url').strip():
            return chunk.get('url')

        # 2. metadataì— urlì´ ìˆìœ¼ë©´ ì¶”ì¶œ
        if chunk.get('metadata'):
            metadata = chunk['metadata']
            if isinstance(metadata, str):
                try:
                    import json
                    metadata = json.loads(metadata)
                except:
                    pass
            if isinstance(metadata, dict) and metadata.get('url'):
                return metadata.get('url')

        # 3. document_idë¡œ documents í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        if chunk.get('document_id'):
            try:
                doc = self.supabase_client.client.table('documents').select('metadata').eq('id', chunk['document_id']).single().execute()
                if doc.data and doc.data.get('metadata'):
                    metadata = doc.data['metadata']
                    if isinstance(metadata, str):
                        import json
                        metadata = json.loads(metadata)
                    if isinstance(metadata, dict) and metadata.get('url'):
                        return metadata.get('url')
            except Exception as e:
                logger.debug(f"Document ì¡°íšŒ ì‹¤íŒ¨ ({chunk['document_id']}): {e}")

        return ""

    def search(self, query: str) -> tuple[str, List[Dict]]:
        """ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ (URL ì™„ë²½ ë³´ì¡´)"""
        try:
            query_embedding = self.embeddings.embed_query(query)
            chunks = self.supabase_client.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold,
                ef_search=self.ef_search
            )

            if not chunks:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                similarity = chunk.get('similarity', 0.0)

                # âœ… URL ì¶”ì¶œ (3ê°€ì§€ ë°©ë²• ì‹œë„)
                url = self._get_chunk_url(chunk)

                # âœ… URL ì™„ë²½ ë³´ì¡´ (ì ˆëŒ€ ì§¤ë¦¬ì§€ ì•Šê²Œ)
                url_section = ""
                if url and url.strip():
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"ìœ ì‚¬ë„: {similarity:.2f}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

    def search_hybrid(self, query: str, use_reranking: bool = None) -> tuple[str, List[Dict]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (PGroonga + pgvector) + ë¦¬ë­í‚¹ + URL ìë™ ì¶”ê°€"""

        if use_reranking is None:
            use_reranking = RERANKER_CONFIG['enabled']

        try:
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)

            # 2. Supabase RPC í˜¸ì¶œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            response = self.supabase_client.client.rpc(
                'hybrid_search_veddy',
                {
                    'query_text': query,
                    'query_embedding': query_embedding,
                    'match_count': self.k * 2 if use_reranking else self.k,
                    'full_text_weight': 0.4,
                    'semantic_weight': 0.6
                }
            ).execute()

            if not response.data:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            chunks = response.data

            # âœ… 3. URL ìë™ ì¶”ê°€ (RPC ê²°ê³¼ì— urlì´ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€)
            logger.info(f"RPC ê²€ìƒ‰ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬ | URL ìë™ ì¶”ê°€ ì‹œì‘")
            for chunk in chunks:
                if not chunk.get('url') or not chunk.get('url').strip():
                    url = self._get_chunk_url(chunk)
                    if url:
                        chunk['url'] = url
                        logger.debug(f"URL ì¶”ê°€ë¨: {chunk.get('title', 'N/A')[:30]} | {url[:50]}...")

            # 4. ë¦¬ë­í‚¹ ì ìš©
            if use_reranking and len(chunks) > 1:
                from services.reranker_service import reranker_service
                logger.info(f"ë¦¬ë­í‚¹ ì „ ì²­í¬ ìˆ˜: {len(chunks)}")
                chunks = reranker_service.rerank(
                    query=query,
                    chunks=chunks,
                    top_k=RERANKER_CONFIG['top_k']
                )
                logger.info(f"ë¦¬ë­í‚¹ í›„ ì²­í¬ ìˆ˜: {len(chunks)}")

            # 5. ì‘ë‹µ í¬ë§·íŒ… (URL ì™„ë²½ ë³´ì¡´)
            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                url = chunk.get('url', '')

                # ë¦¬ë­í¬ ì ìˆ˜ í‘œì‹œ
                if 'rerank_score' in chunk:
                    score = chunk.get('rerank_score', 0.0)
                    score_label = f"ë¦¬ë­í¬: {score:.4f}"
                else:
                    score = chunk.get('score', 0.0)
                    score_label = f"ê´€ë ¨ë„: {score:.4f}"

                # âœ… URL ì™„ë²½ ë³´ì¡´
                url_section = ""
                if url and url.strip():
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"{score_label}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

    def search_multi_topic(self, query: str, topics: list) -> tuple[str, List[Dict]]:
        """ë©€í‹° ì£¼ì œ ê²€ìƒ‰ (ë¹„êµ ëª¨ë“œ) - ê° í† í”½ë³„ ë”°ë¡œ ê²€ìƒ‰ í›„ ë³‘í•©"""

        if not topics or len(topics) < 2:
            return self.search_hybrid(query)

        all_results = []
        all_chunks = []

        for topic in topics:
            search_query = f"{topic} ë² ìŠ¬ë§í¬"
            context, chunks = self.search_hybrid(search_query)

            # ê° ì£¼ì œë³„ë¡œ í—¤ë” ì¶”ê°€
            topic_section = f"\n### ã€{topic}ã€‘\n{context}"
            all_results.append(topic_section)
            all_chunks.extend(chunks)

        # ê²°í•©
        combined_context = "\n---\n".join(all_results)

        return combined_context, all_chunks

# ===== ë² ë”” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì™„ì „ ê°œì„ ) =====

VEDDY_SYSTEM_PROMPT = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## ë„ˆì˜ ì—­í• ê³¼ ì •ì²´ì„±

ì´ë¦„: ë² ë”” (Vessellink's Buddy)
ì„±ê²©: ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•¨
ëª©í‘œ: ë² ìŠ¬ë§í¬ ì§ì›ë“¤ì˜ ì—…ë¬´ íš¨ìœ¨í™”ì™€ ì •ë³´ ì ‘ê·¼ì„± ê°œì„ 
ì „ë¬¸ì„±: ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€

## âš ï¸ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ (CRITICAL)

âœ… ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
1. ê²€ìƒ‰ëœ ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
3. ë¶ˆí™•ì‹¤í•˜ë©´ "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ëª…ì‹œ
4. ë² ìŠ¬ë§í¬ ê´€ë ¨ ì •ë³´ëŠ” ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‹ ë¢°í•˜ì„¸ìš”

âŒ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”:
- "ì•„ë§ˆë„ ~ì¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤"
- "ì¼ë°˜ì ìœ¼ë¡œëŠ” ~ì…ë‹ˆë‹¤"
- "ë² ìŠ¬ë§í¬ì—ì„œëŠ” ~ì„ ì§€ì›í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤"
- ë¬¸ì„œì— ì—†ëŠ” ì¶”ê°€ ì„¤ëª…/í•´ì„

## ë‹µë³€ í¬ë§· ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)

**ì œëª©** (í•œ ì¤„)

1. ì²« ë²ˆì§¸ í¬ì¸íŠ¸ (ê° í¬ì¸íŠ¸ëŠ” ê°œí–‰ í›„ ì‹œì‘)

2. ë‘ ë²ˆì§¸ í¬ì¸íŠ¸ (ê° í¬ì¸íŠ¸ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€)

3. ì„¸ ë²ˆì§¸ í¬ì¸íŠ¸

ã€URL ë° ì°¸ê³  ë¬¸ì„œã€‘

ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
"""

TABLE_MODE_PROMPT = """
ğŸš¨ í‘œ í˜•ì‹ ë‹µë³€ ëª¨ë“œ í™œì„±í™” - ì ˆëŒ€ ì¤€ìˆ˜ ğŸš¨

1. ë‹µë³€ì˜ ì²« ì¤„ì€ ì œëª©ë§Œ ì‘ì„±
2. ì œëª© ë‹¤ìŒ ì¤„ë¶€í„° ì¦‰ì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ ì‹œì‘
3. ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3.)ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€

ã€í‘œ í¬ë§· ì˜ˆì‹œã€‘

| í•­ëª© | ì„¤ëª… |
|------|------|
| ì²« ë²ˆì§¸ | ë‚´ìš© |
| ë‘ ë²ˆì§¸ | ë‚´ìš© |
"""

USER_MESSAGE_TEMPLATE = """ã€ì´ì „ ëŒ€í™” ë§¥ë½ã€‘
{history}

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘
{query}

ã€âš ï¸ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ - CRITICALã€‘

âœ… ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
1. ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ìœ¼ë©´ "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ëª…ì‹œ
3. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
4. URLì€ ë°˜ë“œì‹œ ì „ì²´ê²½ë¡œ í¬í•¨

âŒ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”:
- "ì•„ë§ˆë„ ~ì¼ ê²ƒì…ë‹ˆë‹¤"
- "ì¼ë°˜ì ìœ¼ë¡œ ~í•©ë‹ˆë‹¤"
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ ì¶”ê°€

ã€ë‹µë³€ í¬ë§· - í•„ìˆ˜ ì¤€ìˆ˜ã€‘

**ì œëª©** (í•œ ì¤„ - í•µì‹¬ì„ ëª…í™•íˆ)

1. ì²« ë²ˆì§¸ í¬ì¸íŠ¸ (ê°œí–‰ í›„ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘)

2. ë‘ ë²ˆì§¸ í¬ì¸íŠ¸ (ê° í¬ì¸íŠ¸ ì‚¬ì´ì— ë¹ˆ ì¤„)

3. ì„¸ ë²ˆì§¸ í¬ì¸íŠ¸

ã€URL ë° ì°¸ê³  ë¬¸ì„œ - í•„ìˆ˜ã€‘

ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ/ìœ ì§€]"""

TABLE_USER_MESSAGE_TEMPLATE = """ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘
{query}

ã€â€¼ï¸ í‘œ í˜•ì‹ ë‹µë³€ í•„ìˆ˜ã€‘

ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ã€í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ã€‘
- ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì •ë³´ë§Œ ì‚¬ìš©
- ë¬¸ì„œì— ì—†ìœ¼ë©´ "í™•ì¸í•  ìˆ˜ ì—†ìŒ" í‘œê¸°
- URL ì „ì²´ê²½ë¡œ ìœ ì§€ (ì ˆëŒ€ ì¤„ì´ì§€ ë§ ê²ƒ)"""

COMPARISON_CONTEXT_TEMPLATE = """ã€ì´ì „ ëŒ€í™” ë§¥ë½ã€‘
{history}

ã€ë¹„êµ ëŒ€ìƒã€‘
{topics}

ã€í˜„ì¬ ì§ˆë¬¸ã€‘
{query}

ã€ì§ˆë¬¸ ì˜ë„ã€‘
ì‚¬ìš©ìê°€ ì—¬ëŸ¬ í•­ëª©ì„ ë¹„êµí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë¹„êµí•˜ì„¸ìš”."""

COMPARISON_USER_TEMPLATE = """ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ë¹„êµ ë¶„ì„ ì§€ì¹¨ - í•„ìˆ˜ã€‘

âœ… ê° í•­ëª©ë³„ë¡œ:
1. ì •ì˜/ëª©ì  ëª…í™•íˆ êµ¬ë¶„
2. ë²”ìœ„/ì ìš© ëŒ€ìƒ ëª…ì‹œ
3. ë² ìŠ¬ë§í¬ ì§€ì› ì—¬ë¶€ (ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‹ ë¢°)
4. ê³µí†µì  ë° ì°¨ì´ì  ì •ë¦¬

âŒ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€:
- ê° í•­ëª©ì€ ê²€ìƒ‰ëœ ë¬¸ì„œë§Œ ì‚¬ìš©
- ë¬¸ì„œì— ì—†ëŠ” ë¹„êµëŠ” ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€"""

# ===== LangChain 1.0 RAG ì„œë¹„ìŠ¤ (ì™„ì „ ê°œì„ ) =====

class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (Phase 3-A Final ì™„ì „ ì™„ì„±)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")
        logger.info(f"ğŸ“Š Config ì ìš©: ef_search={VECTOR_SEARCH_CONFIG['ef_search']}, threshold={VECTOR_SEARCH_CONFIG['similarity_threshold']}")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. LLM
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê°œì„ ë¨)
        self.base_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", USER_MESSAGE_TEMPLATE)
        ])

        self.table_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT + TABLE_MODE_PROMPT),
            ("user", TABLE_USER_MESSAGE_TEMPLATE)
        ])

        # ë¹„êµ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.comparison_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", COMPARISON_CONTEXT_TEMPLATE + "\n\n" + COMPARISON_USER_TEMPLATE)
        ])

        # ë¹„êµ + í…Œì´ë¸” í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡¬í”„íŠ¸
        self.comparison_table_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT + TABLE_MODE_PROMPT),
            ("user", COMPARISON_CONTEXT_TEMPLATE + "\n\n" + COMPARISON_USER_TEMPLATE)
        ])

        # 4. Retriever ì‹±ê¸€í†¤
        self._retriever = None

        logger.info("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ ì™„ì „ ê°œì„  + URL ìë™ ì¶”ê°€ + History)")

    @property
    def retriever(self) -> SupabaseRetriever:
        """Retriever ì‹±ê¸€í†¤ (ë©”ëª¨ë¦¬ íš¨ìœ¨)"""
        if self._retriever is None:
            self._retriever = SupabaseRetriever(
                embeddings=self.embeddings,
                supabase_client=supabase_service,
                k=5
            )
        return self._retriever

    def _safe_format(self, template: ChatPromptTemplate, **kwargs) -> list:
        """ì•ˆì „í•œ format_messages (ì„ íƒì  íŒŒë¼ë¯¸í„° ì²˜ë¦¬)"""
        required_vars = template.input_variables
        safe_kwargs = {}

        for var in required_vars:
            if var in kwargs:
                safe_kwargs[var] = kwargs[var]
            else:
                safe_kwargs[var] = ""  # ê¸°ë³¸ê°’: ë¹ˆ ë¬¸ìì—´

        return template.format_messages(**safe_kwargs)

    def _normalize_response(self, response: str) -> str:
        """âœ… ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ê·œí™” (ìëª¨ ë¶„ë¦¬ ë³µêµ¬)"""
        # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicode_normalize('NFC', response)

        # 2. ì¤„ë°”ê¿ˆ í†µì¼
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        # 3. 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        text = re.sub(r'\n{3,}', '\n\n', text)

        # 4. ê° ì¤„ ê³µë°± ì •ë¦¬
        lines = [re.sub(r' +', ' ', line.rstrip()) for line in text.split('\n')]

        # 5. ìµœì¢… ì •ë¦¬
        return '\n'.join(lines).strip()

    # services/langchain_rag_service.py
    def process_query_streaming(
            self,
            user_id: str,
            query: str,
            table_mode: bool = False,
            supabase_client: Optional[SupabaseService] = None,
            history: str = None,
            comparison_info: dict = None,
            conversation_context: List[Dict] = None  # âœ… ì¶”ê°€
    ) -> Generator[str, None, None]:
        """
        RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í…Œì´ë¸” ëª¨ë“œ + ë¹„êµ ëª¨ë“œ ì¡°í•© ê°€ëŠ¥)

        ì•„í‚¤í…ì²˜:
        1ï¸âƒ£ Step 1: ê²€ìƒ‰ ë°©ì‹ ê²°ì • (mode ê¸°ë°˜) â†’ context ìƒì„±
        2ï¸âƒ£ Step 2: í”„ë¡¬í”„íŠ¸ ì„ íƒ (table_mode ê¸°ë°˜) â†’ ë…ë¦½ì  ì ìš©
        """

        try:
            client = supabase_client if supabase_client else supabase_service

            if comparison_info is None:
                comparison_info = {"is_comparison": False, "topics": []}

            # ğŸ¯ Step 1: ê²€ìƒ‰ ë°©ì‹ ê²°ì • (ëª¨ë“œ ê¸°ë°˜)
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ ë¹„êµ ëª¨ë“œ vs ì¼ë°˜ ëª¨ë“œ (ë…ë¦½ì )         â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            is_comparison = comparison_info.get("is_comparison", False)
            topics = comparison_info.get("topics", [])

            if is_comparison and topics and len(topics) >= 2:
                # âœ… ë¹„êµ ëª¨ë“œ: ê° í† í”½ë³„ ê²€ìƒ‰
                logger.info("ğŸ”„ ë¹„êµ ëª¨ë“œ ê²€ìƒ‰", extra={
                    "topics": topics,
                    "confidence": comparison_info.get("confidence", "N/A")
                })
                context_text, raw_chunks = self.retriever.search_multi_topic(
                    query, topics
                )
                is_in_comparison_mode = True

            else:
                # âœ… ì¼ë°˜ ëª¨ë“œ: ì¼ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                logger.info("ğŸ“ ì¼ë°˜ ëª¨ë“œ ê²€ìƒ‰")
                context_text, raw_chunks = self.retriever.search_hybrid(query)
                is_in_comparison_mode = False

            # ğŸ¯ Step 2: í”„ë¡¬í”„íŠ¸ ì„ íƒ (table_mode ê¸°ë°˜) â† ë…ë¦½ì 
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ í…Œì´ë¸” í˜•ì‹ ì—¬ë¶€ (ëª¨ë“œì™€ ë¬´ê´€)          â”‚
            # â”‚ ì–´ë–¤ ê²€ìƒ‰ì´ë“  í…Œì´ë¸”ë¡œ í‘œí˜„ ê°€ëŠ¥        â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            prompt_template = self._select_prompt_template(
                table_mode=table_mode,
                is_comparison=is_in_comparison_mode,
                topics=topics if is_in_comparison_mode else []
            )

            logger.info("ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„ íƒ", extra={
                "table_mode": table_mode,
                "is_comparison": is_in_comparison_mode
            })

            # âœ… Step 3: ë©”ì‹œì§€ í¬ë§·
            messages = self._safe_format(
                prompt_template,
                context=context_text,
                query=query,
                history=history or "",
                topics=", ".join(topics) if is_in_comparison_mode else ""
            )

            # âœ… Step 4: ìŠ¤íŠ¸ë¦¬ë°
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = unicode_normalize('NFC', chunk.content)
                    yield token

            logger.info("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ", extra={
                "table_mode": table_mode,
                "is_comparison": is_in_comparison_mode
            })

        except Exception as e:
            logger.error(f"âŒ RAG ì˜¤ë¥˜: {e}", exc_info=True)
            yield f"\n\n[ì˜¤ë¥˜]\n{str(e)}"

        # âœ… ìƒˆ ë©”ì„œë“œ: í”„ë¡¬í”„íŠ¸ ì„ íƒ ë¡œì§
    def _select_prompt_template(
            self,
            table_mode: bool,
            is_comparison: bool,
            topics: List[str] = None
    ) -> ChatPromptTemplate:
        """
        í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ (í…Œì´ë¸” + ëª¨ë“œ ì¡°í•©)

        ë¡œì§:
        1. table_mode í™•ì¸ â†’ base ì„ íƒ (table vs normal)
        2. is_comparison í™•ì¸ â†’ í”„ë¡¬í”„íŠ¸ ë‚´ìš© ì¶”ê°€
        """

        # âœ… ë¹„êµ ëª¨ë“œ + í…Œì´ë¸” í˜•ì‹ (í•˜ì´ë¸Œë¦¬ë“œ)
        if is_comparison and table_mode:
            logger.info(f"ğŸ“‹ ë¹„êµ + í…Œì´ë¸” í”„ë¡¬í”„íŠ¸ ì„ íƒ (ì£¼ì œ: {topics})")
            return self.comparison_table_prompt_template

        # âœ… ë¹„êµ ëª¨ë“œ + ì¼ë°˜ í˜•ì‹
        elif is_comparison:
            logger.info(f"ğŸ“‹ ë¹„êµ í”„ë¡¬í”„íŠ¸ ì„ íƒ (ì£¼ì œ: {topics})")
            return self.comparison_prompt_template

        # âœ… ì¼ë°˜ ëª¨ë“œ + í…Œì´ë¸” í˜•ì‹
        elif table_mode:
            logger.info("ğŸ“‹ í…Œì´ë¸” í”„ë¡¬í”„íŠ¸ ì„ íƒ")
            return self.table_prompt_template

        # âœ… ì¼ë°˜ ëª¨ë“œ + ì¼ë°˜ í˜•ì‹
        else:
            logger.info("ğŸ“‹ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ì„ íƒ")
            return self.base_prompt_template

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
