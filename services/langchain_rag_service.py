# services/langchain_rag_service.py (LangChain 1.0 + ë² ë”” í”„ë¡¬í”„íŠ¸)

from typing import List, Dict, Any, Generator

# LangChain 1.0 Import
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool

from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service
from config import OPENAI_API_KEY


# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====
class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)


# ===== Supabase Retriever =====
class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼"""

    def __init__(self, embeddings: Embeddings, k: int = 5, threshold: float = 0.3):
        self.embeddings = embeddings
        self.k = k
        self.threshold = threshold

    def search(self, query: str) -> str:
        """ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            query_embedding = self.embeddings.embed_query(query)
            chunks = supabase_service.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold
            )

            if not chunks:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')

                context_parts.append(
                    f"ğŸ“„ [ë¬¸ì„œ {i}] {title}\n{content}\nğŸ“ ì¶œì²˜: {source}"
                )

            return "\n\n".join(context_parts)

        except Exception as e:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"


# ===== ë² ë”” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ =====
VEDDY_SYSTEM_PROMPT = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## ë„ˆì˜ ì—­í• ê³¼ ì •ì²´ì„±
- ì´ë¦„: ë² ë”” (Vessellink's Buddy)
- ì„±ê²©: ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•¨
- ëª©í‘œ: ë² ìŠ¬ë§í¬ ì§ì›ë“¤ì˜ ì—…ë¬´ íš¨ìœ¨í™”ì™€ ì •ë³´ ì ‘ê·¼ì„± ê°œì„ 
- ì „ë¬¸ì„±: ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€

## ë‹µë³€ì˜ ì›ì¹™ (ì ˆëŒ€ ì¤€ìˆ˜)
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ë§Œ ì œê³µ**
   - ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ë‹µë³€
   - ë¬¸ì„œì— ì—†ëŠ” ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ì§€ì‹ì€ ì œê³µí•˜ì§€ ë§ ê²ƒ

2. **êµ¬ì¡°í™”ëœ ë‹µë³€ í¬ë§·**
   - [ë‹µë³€ ë³¸ë¬¸] â†’ ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€
   - [ì°¸ê³  ë¬¸ì„œ] â†’ "X ë¬¸ì„œ, Y í•­ëª©" í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ëª…ì‹œ
   - [ì¶”ê°€ ì •ë³´] (í•„ìš”ì‹œ) â†’ ì—°ê´€ ê·œì •ì´ë‚˜ ë‹´ë‹¹ì ì •ë³´

3. **í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€**
   - ë¶ˆí™•ì‹¤í•œ ê²½ìš°: "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
   - ë¶€ë¶„ ì¼ì¹˜: "ë‹¤ìŒ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì •í™•í•œ ë‚´ìš©ì€ [ë¬¸ì„œëª…]ì„ ì°¸ê³ í•˜ì„¸ìš”"
   - ë³µìˆ˜ ë‹µë³€: "ë‹¤ìŒ ì—¬ëŸ¬ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤: 1) ... 2) ... ìì„¸í•œ ë‚´ìš©ì€ ë¬¸ì„œ ì°¸ê³ "

4. **í†¤ & ë§¤ë„ˆ**
   - ë†’ì„ë§ ì‚¬ìš© (ì¡´ëŒ“ê¸€)
   - ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ í‘œí˜„ ("ë„ì›€ì´ ë˜ê¸¸ ë°”ëë‹ˆë‹¤", "í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?")
   - ê³¼ë„í•œ ì´ëª¨ì§€ë‚˜ ë°˜ë§ ê¸ˆì§€
   - ì—…ë¬´ì ì´ë©´ì„œë„ ë”°ëœ»í•œ í†¤ ìœ ì§€

## ì²˜ë¦¬í•´ì•¼ í•  ìƒí™©ë³„ ì‘ë‹µ

### ìƒí™©1: ë¬¸ì„œì—ì„œ ì™„ë²½í•˜ê²Œ ì°¾ì€ ê²½ìš°
[ëª…í™•í•œ ë‹µë³€ ë‚´ìš©]
ì°¸ê³  ë¬¸ì„œ: [êµ¬ì²´ì  ë¬¸ì„œëª…] > [ì„¹ì…˜]

### ìƒí™©2: ë¬¸ì„œì— ì—†ëŠ” ê²½ìš°
ì£„ì†¡í•˜ì§€ë§Œ, í˜„ì¬ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ë” ìì„¸í•œ ë‚´ìš©ì€ [ë‹´ë‹¹ ë¶€ì„œ] ë˜ëŠ” [ë‹´ë‹¹ìëª…]ì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.

### ìƒí™©3: ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
ë‹¤ìŒê³¼ ê°™ì€ ê´€ë ¨ ì •ë³´ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:
1. [ë¬¸ì„œ1]ì—ì„œ: ...
2. [ë¬¸ì„œ2]ì—ì„œ: ...
ì–´ëŠ ì •ë³´ê°€ ë” í•„ìš”í•˜ì‹ ì§€ ì•Œë ¤ì£¼ì„¸ìš”.

### ìƒí™©4: ì§ˆë¬¸ì´ ëª¨í˜¸í•œ ê²½ìš°
ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?
ì˜ˆë¥¼ ë“¤ì–´, [ì¶”ì¸¡ë˜ëŠ” ì„¸ë¶€ ì‚¬í•­]ì— ëŒ€í•´ ë¬»ëŠ” ê±´ê°€ìš”?

## ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­
âŒ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë³´ì¶©
âŒ í™•ì‹¤í•˜ì§€ ì•Šì€ ì¶œì²˜ ëª…ì‹œ
âŒ ê³¼ë„í•˜ê²Œ ê¸¸ê±°ë‚˜ ìš”ì•½ë˜ì§€ ì•Šì€ ë‹µë³€
âŒ ë§ˆí¬ë‹¤ìš´ ì˜¤ë²„í¬ë§·íŒ… (í•„ìš”í•œ ë§Œí¼ë§Œ)
âŒ ê°œì¸ ì˜ê²¬ì´ë‚˜ ì¶”ì²œ (ë¬¸ì„œ ê¸°ë°˜ë§Œ)"""


# ===== LangChain 1.0 RAG ì„œë¹„ìŠ¤ =====
class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (ë² ë”” í”„ë¡¬í”„íŠ¸ ì ìš©)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        print("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. Retriever
        self.retriever = SupabaseRetriever(
            embeddings=self.embeddings,
            k=5,
            threshold=0.3
        )

        # 3. LLM (í•­ìƒ ì´ˆê¸°í™”)
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 4. Tool ì •ì˜
        @tool
        def search_knowledge_base(query: str) -> str:
            """ë² ìŠ¬ë§í¬ ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
            return self.retriever.search(query)

        self.tools = [search_knowledge_base]

        # 5. Agent ìƒì„± ì‹œë„ (ì„ íƒì‚¬í•­)
        self.agent = None
        try:
            self.agent = create_agent(
                model="openai:gpt-4o-mini",
                tools=self.tools,
                system_prompt=VEDDY_SYSTEM_PROMPT
            )
            print("âœ… LangChain 1.0 Agent ì‚¬ìš©")
        except Exception as e:
            print(f"âš ï¸ create_agent ì‹¤íŒ¨, ì§ì ‘ LLM í˜¸ì¶œ ëª¨ë“œ ({e})")

        print("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ")

    def process_query(self, user_id: str, query: str) -> Dict[str, Any]:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬ (ì¼ë°˜ ì‘ë‹µ)"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            context_text = self.retriever.search(query)

            # 2. ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„± (ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ í˜•ì‹ ìœ ì§€)
            user_message = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {query}

(ì¶œì²˜ëŠ” í•­ìƒ ëª…ì‹œí•´ì£¼ì„¸ìš”)"""

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", VEDDY_SYSTEM_PROMPT),
                ("user", user_message)
            ])
            messages = prompt.format_messages()

            # 4. LLM í˜¸ì¶œ
            response = self.llm.invoke(messages)
            ai_response = response.content

            # 5. ë©”ì‹œì§€ ì €ì¥
            supabase_service.save_message(
                user_id=user_id,
                user_query=query,
                ai_response=ai_response,
                source_chunk_ids=[],
                usage={}
            )

            return {
                "user_query": query,
                "ai_response": ai_response,
                "source_chunks": [],
                "usage": {}
            }

        except Exception as e:
            print(f"âŒ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def process_query_streaming(self, user_id: str, query: str) -> Generator[str, None, None]:
        """RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ë² ë”” í”„ë¡¬í”„íŠ¸ ì ìš©)"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            context_text = self.retriever.search(query)

            # 2. ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
            user_message = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {query}

(ì¶œì²˜ëŠ” í•­ìƒ ëª…ì‹œí•´ì£¼ì„¸ìš”)"""

            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", VEDDY_SYSTEM_PROMPT),
                ("user", user_message)
            ])
            messages = prompt.format_messages()

            # 4. ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
            full_response = ""
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = chunk.content
                    full_response += token
                    yield token

            # 5. ë©”ì‹œì§€ ì €ì¥
            supabase_service.save_message(
                user_id=user_id,
                user_query=query,
                ai_response=full_response,
                source_chunk_ids=[],
                usage={}
            )

        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"[ì˜¤ë¥˜] {str(e)}"


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
