# services/langchain_rag_service.py (LangChain 1.0)

from typing import List, Dict, Any, Generator

# ===== LangChain 1.0 Import =====
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool

# âš ï¸ LangChain 1.0ì˜ ìƒˆë¡œìš´ Agent API
from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service
from config import OPENAI_API_KEY


# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====
class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)


# ===== Tool ì •ì˜ (LangChain 1.0 @tool ë°ì½”ë ˆì´í„°) =====
class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼"""

    def __init__(self, embeddings: Embeddings, k: int = 5, threshold: float = 0.3):
        self.embeddings = embeddings
        self.k = k
        self.threshold = threshold

    def search(self, query: str) -> str:
        """ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embeddings.embed_query(query)

            # Supabase ê²€ìƒ‰
            chunks = supabase_service.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold
            )

            if not chunks:
                return "âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                similarity = chunk.get('similarity', 0.0)

                context_parts.append(
                    f"ğŸ“„ [ë¬¸ì„œ {i}] {title}\n"
                    f"ìœ ì‚¬ë„: {similarity:.2f}\n"
                    f"{content}\n"
                    f"ğŸ“ ì¶œì²˜: {source}"
                )

            return "\n\n".join(context_parts)

        except Exception as e:
            return f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (create_agent ì‚¬ìš©)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        print("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. Retriever
        self.retriever = SupabaseRetriever(
            embeddings=self.embeddings,
            k=5,
            threshold=0.3
        )

        # ===== ğŸ”¥ í•µì‹¬: LLMì„ í•­ìƒ ë¨¼ì € ì´ˆê¸°í™” =====
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 3. Tool ì •ì˜
        @tool
        def search_knowledge_base(query: str) -> str:
            """ë² ìŠ¬ë§í¬ ì‚¬ë‚´ ë¬¸ì„œ(Confluence)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
            return self.retriever.search(query)

        self.tools = [search_knowledge_base]

        # 4. Agent ìƒì„± ì‹œë„ (ì„ íƒì‚¬í•­)
        try:
            self.agent = create_agent(
                model="openai:gpt-4o-mini",
                tools=self.tools,
                system_prompt="""ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## í•µì‹¬ ì›ì¹™
1. **ë°˜ë“œì‹œ search_knowledge_base ë„êµ¬ë¥¼ ë¨¼ì € ì‚¬ìš©**í•´ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê²€ìƒ‰
2. ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ (í• ë£¨ì‹œë„¤ì´ì…˜ ê¸ˆì§€)
3. ë‹µë³€ ì‹œ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œê¸° (ì˜ˆ: [ë¬¸ì„œ 1] ì°¸ê³ )
4. ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•˜ë©´ "ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì •ì§í•˜ê²Œ ë‹µë³€
5. ì¹œì ˆí•˜ê³  ëª…í™•í•œ í•œêµ­ì–´ ì‚¬ìš©

## ë‹µë³€ í˜•ì‹
- í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œì‹œ
- ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ì¶œì²˜ ëª…ì‹œ
- ì¶”ê°€ ì •ë³´ë‚˜ ê´€ë ¨ ì ˆì°¨ê°€ ìˆìœ¼ë©´ ì•ˆë‚´"""
            )
            print("âœ… create_agent ì‚¬ìš©")
        except Exception as e:
            print(f"âš ï¸ create_agent ì‹¤íŒ¨, LLM ì§ì ‘ ì‚¬ìš© ëª¨ë“œ: {e}")
            self.agent = None

        print("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ")

    def process_query(self, user_id: str, query: str) -> Dict[str, Any]:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬ (ì¼ë°˜ ì‘ë‹µ)"""
        try:
            if self.agent:
                # Agent ì‚¬ìš©
                result = self.agent.invoke({
                    "messages": [{"role": "user", "content": query}]
                })
                ai_response = result["messages"][-1]["content"]
            else:
                # Fallback: ì§ì ‘ ê²€ìƒ‰ + LLM í˜¸ì¶œ
                context = self.retriever.search(query)
                prompt = ChatPromptTemplate.from_messages([
                    ("system", "ë„ˆëŠ” ë² ë””(VEDDY)ì•¼. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´."),
                    ("user", f"ì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {query}")
                ])
                messages = prompt.format_messages()
                response = self.llm.invoke(messages)
                ai_response = response.content

            # ë©”ì‹œì§€ ì €ì¥
            supabase_service.save_message(
                user_id=user_id,
                user_query=query,
                ai_response=ai_response,
                source_chunk_ids=[],
                usage={}
            )

            return {
                "user_query": query,
                "ai_response": ai_response,
                "source_chunks": [],
                "usage": {}
            }

        except Exception as e:
            print(f"âŒ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def process_query_streaming(self, user_id: str, query: str) -> Generator[str, None, None]:
        """RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            context = self.retriever.search(query)

            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ë„ˆëŠ” ë² ë””(VEDDY)ì•¼. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´."),
                ("user", f"ì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {query}")
            ])
            messages = prompt.format_messages()

            # 3. ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
            full_response = ""
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = chunk.content
                    full_response += token
                    yield token

            # 4. ë©”ì‹œì§€ ì €ì¥
            supabase_service.save_message(
                user_id=user_id,
                user_query=query,
                ai_response=full_response,
                source_chunk_ids=[],
                usage={}
            )

        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"[ì˜¤ë¥˜] {str(e)}"


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
