# backend/services/langchain_rag_service.py (âœ… config í†µí•© ì™„ë£Œ)

import re
from unicodedata import normalize as unicode_normalize
from typing import List, Dict, Any, Generator, Optional
from datetime import datetime

# LangChain 1.0 Import
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool
from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service, SupabaseService
from config import OPENAI_API_KEY, VECTOR_SEARCH_CONFIG  # âœ… config í†µí•©

# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====
class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)

# ===== Supabase Retriever (config í†µí•©) =====
class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼ (URL í¬í•¨, config ê¸°ë°˜)"""

    def __init__(self, embeddings: Embeddings, supabase_client: SupabaseService,
                 k: int = 5, threshold: float = None, ef_search: int = None):
        self.embeddings = embeddings
        self.supabase_client = supabase_client

        # âœ… configì—ì„œ ê¸°ë³¸ê°’ ìë™ ì ìš©
        self.k = k
        self.threshold = threshold or VECTOR_SEARCH_CONFIG['similarity_threshold']
        self.ef_search = ef_search or VECTOR_SEARCH_CONFIG['ef_search']

        print(f"ğŸ” Retriever ì´ˆê¸°í™” | k={self.k} | threshold={self.threshold} | ef_search={self.ef_search}")

    def search(self, query: str) -> tuple[str, List[Dict]]:
        """ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ (URL ì™„ë²½ ë³´ì¡´)"""
        try:
            query_embedding = self.embeddings.embed_query(query)
            chunks = self.supabase_client.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold,
                ef_search=self.ef_search
            )

            if not chunks:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                url = chunk.get('url', '')
                similarity = chunk.get('similarity', 0.0)

                # âœ… URL ë³´ì¡´
                url_section = ""
                if url and url.strip():
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"ìœ ì‚¬ë„: {similarity:.2f}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

    def search_hybrid(self, query: str) -> tuple[str, List[Dict]]:
        """
        ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (PGroonga + pgvector)
        RPC í•¨ìˆ˜ í˜¸ì¶œ
        """
        try:
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)

            # 2. Supabase RPC í˜¸ì¶œ (hybrid_search_veddy í•¨ìˆ˜)
            response = self.supabase_client.client.rpc(
                'hybrid_search_veddy',
                {
                    'query_text': query,
                    'query_embedding': query_embedding,
                    'match_count': self.k,
                    'full_text_weight': 0.4,  # í‚¤ì›Œë“œ 40%
                    'semantic_weight': 0.6    # ì˜ë¯¸ 60%
                }
            ).execute()

            if not response.data:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            # 3. ì‘ë‹µ í¬ë§·íŒ…
            chunks = response.data
            context_parts = []

            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                score = chunk.get('score', 0.0)

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"ê´€ë ¨ë„: {score:.4f}\n"
                    f"ì¶œì²˜: {source}\n"
                    f"ë‚´ìš©:\n{content}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

# ===== ë² ë”” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ =====
VEDDY_SYSTEM_PROMPT = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## ë„ˆì˜ ì—­í• ê³¼ ì •ì²´ì„±
ì´ë¦„: ë² ë”” (Vessellink's Buddy)
ì„±ê²©: ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•¨
ëª©í‘œ: ë² ìŠ¬ë§í¬ ì§ì›ë“¤ì˜ ì—…ë¬´ íš¨ìœ¨í™”ì™€ ì •ë³´ ì ‘ê·¼ì„± ê°œì„ 
ì „ë¬¸ì„±: ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€

## ë‹µë³€ í¬ë§· ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)
âœ… í•„ìˆ˜ í¬ë§·:
1. **ì œëª© (1ì¤„)**
2. **ë¹ˆ ì¤„**
3. **ë³¸ë¬¸ (ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸)**
   ê° ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.
   ê° ë²ˆí˜¸ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë°˜ë“œì‹œ ì¶”ê°€í•˜ì„¸ìš”.
4. **ì°¸ê³  ë¬¸ì„œ ì„¹ì…˜**
   ğŸ“š ì°¸ê³  ë¬¸ì„œ:
   - ë¬¸ì„œëª… > (ì„¹ì…˜ëª…)
   URL: https://...

í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?"""

TABLE_MODE_PROMPT = """
ğŸš¨ í‘œ í˜•ì‹ ë‹µë³€ ëª¨ë“œ í™œì„±í™” - ì ˆëŒ€ ì¤€ìˆ˜ ğŸš¨
**ì‚¬ìš©ìê°€ í‘œ ëª¨ë“œë¥¼ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:**
1. ë‹µë³€ì˜ ì²« ì¤„ì€ ì œëª©ë§Œ ì‘ì„±
2. ì œëª© ë‹¤ìŒ ì¤„ë¶€í„° ì¦‰ì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ ì‹œì‘
3. ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3.)ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€

| í•­ëª© | ì„¤ëª… |
|------|------|
| ê°’1 | ë‚´ìš©1 |
"""

USER_MESSAGE_TEMPLATE = """ì•„ë˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘
{query}

ã€ë‹µë³€ ì‘ì„± ì§€ì¹¨ - ë§¤ìš° ì¤‘ìš”!ã€‘
1. ë°˜ë“œì‹œ ìœ„ì˜ "ë‹µë³€ í¬ë§· ê·œì¹™"ì„ ë”°ë¼ ì‘ì„±í•˜ì„¸ìš”
2. ì œëª©ì€ í•œ ì¤„ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”
3. ì œëª© ë‹¤ìŒì—ëŠ” ë°˜ë“œì‹œ ë¹ˆ ì¤„(ê°œí–‰)ì„ ì¶”ê°€í•˜ì„¸ìš”
4. ë³¸ë¬¸ì€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3., ...)ë¡œ êµ¬ì„±í•˜ì„¸ìš”
5. ê° ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”"""

TABLE_USER_MESSAGE_TEMPLATE = """ì•„ë˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘
{query}

ã€â€¼ï¸ í‘œ í˜•ì‹ ë‹µë³€ í•„ìˆ˜ã€‘
ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

# ===== LangChain 1.0 RAG ì„œë¹„ìŠ¤ =====
class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (config í†µí•©)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        print("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")
        print(f"ğŸ“Š Config ì ìš©: ef_search={VECTOR_SEARCH_CONFIG['ef_search']}, threshold={VECTOR_SEARCH_CONFIG['similarity_threshold']}")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. LLM
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.base_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", USER_MESSAGE_TEMPLATE)
        ])
        self.table_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT + TABLE_MODE_PROMPT),
            ("user", TABLE_USER_MESSAGE_TEMPLATE)
        ])

        print("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ (config í†µí•©)")

    def _normalize_response(self, response: str) -> str:
        """âœ… ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ê·œí™” (ìëª¨ ë¶„ë¦¬ ë³µêµ¬)"""
        import re
        from unicodedata import normalize as unicode_normalize

        # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicode_normalize('NFC', response)
        # 2. ì¤„ë°”ê¿ˆ í†µì¼
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        # 3. 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        # 4. ê° ì¤„ ê³µë°± ì •ë¦¬
        lines = [re.sub(r' +', ' ', line.rstrip()) for line in text.split('\n')]
        # 5. ìµœì¢… ì •ë¦¬
        return '\n'.join(lines).strip()

    def process_query(
            self,
            user_id: str,
            query: str,
            table_mode: bool = False,
            supabase_client: Optional[SupabaseService] = None
    ) -> Dict[str, Any]:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            client = supabase_client if supabase_client else supabase_service

            # âœ… config ê¸°ë°˜ Retriever (í•˜ë“œì½”ë”© ì œê±°)
            retriever = SupabaseRetriever(
                embeddings=self.embeddings,
                supabase_client=client,
                k=5
                # threshold, ef_searchëŠ” configì—ì„œ ìë™ ì ìš©
            )

            # 2. ë¬¸ì„œ ê²€ìƒ‰
            context_text, raw_chunks = retriever.search(query)

            # 3. í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_template = self.table_prompt_template if table_mode else self.base_prompt_template

            # 4. LLM í˜¸ì¶œ
            messages = prompt_template.format_messages(context=context_text, query=query)
            response = self.llm.invoke(messages)
            ai_response = self._normalize_response(response.content)

            # 5. ì†ŒìŠ¤ ID ì¶”ì¶œ
            source_chunk_ids = [chunk.get('id') for chunk in raw_chunks if chunk.get('id')]

            return {
                "user_query": query,
                "ai_response": ai_response,
                "source_chunks": raw_chunks,
                "source_chunk_ids": source_chunk_ids,
                "usage": {}
            }

        except Exception as e:
            print(f"âŒ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def process_query_streaming(
            self,
            user_id: str,
            query: str,
            table_mode: bool = False,
            supabase_client: Optional[SupabaseService] = None
    ) -> Generator[str, None, None]:
        """RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        try:
            client = supabase_client if supabase_client else supabase_service

            # âœ… config ê¸°ë°˜ Retriever (í•˜ë“œì½”ë”© ì œê±°)
            retriever = SupabaseRetriever(
                embeddings=self.embeddings,
                supabase_client=client,
                k=5
                # threshold, ef_searchëŠ” configì—ì„œ ìë™ ì ìš©
            )

            # 2. ë¬¸ì„œ ê²€ìƒ‰
            context_text, raw_chunks = retriever.search(query)

            # 3. í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_template = self.table_prompt_template if table_mode else self.base_prompt_template
            print(f"[RAG] table_mode: {table_mode}")

            # 4. ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
            messages = prompt_template.format_messages(context=context_text, query=query)
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = unicode_normalize('NFC', chunk.content)
                    yield token

        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ]\n{str(e)}"

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
