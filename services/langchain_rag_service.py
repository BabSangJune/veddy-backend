# backend/services/langchain_rag_service.py (âœ… ìµœì¢… ìˆ˜ì • - ì €ì¥ ì œê±°, ë°˜í™˜ë§Œ)

import re
from unicodedata import normalize as unicode_normalize
from typing import List, Dict, Any, Generator, Optional
from datetime import datetime

# LangChain 1.0 Import
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool
from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service, SupabaseService
from config import OPENAI_API_KEY

# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====

class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)

# ===== Supabase Retriever =====

class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼ (URL í¬í•¨)"""

    def __init__(self, embeddings: Embeddings, supabase_client: SupabaseService, k: int = 5, threshold: float = 0.3):
        self.embeddings = embeddings
        self.supabase_client = supabase_client
        self.k = k
        self.threshold = threshold

    def search(self, query: str) -> tuple[str, List[Dict]]:
        """ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ (URL ì™„ë²½ ë³´ì¡´)"""
        try:
            query_embedding = self.embeddings.embed_query(query)
            chunks = self.supabase_client.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold
            )

            if not chunks:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                url = chunk.get('url', '')
                similarity = chunk.get('similarity', 0.0)

                # âœ… URL ë³´ì¡´ (ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€)
                url_section = ""
                if url and url.strip():
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"ìœ ì‚¬ë„: {similarity:.2f}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

# ===== ë² ë”” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë™ì¼) =====

VEDDY_SYSTEM_PROMPT = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## ë„ˆì˜ ì—­í• ê³¼ ì •ì²´ì„±

ì´ë¦„: ë² ë”” (Vessellink's Buddy)

ì„±ê²©: ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•¨

ëª©í‘œ: ë² ìŠ¬ë§í¬ ì§ì›ë“¤ì˜ ì—…ë¬´ íš¨ìœ¨í™”ì™€ ì •ë³´ ì ‘ê·¼ì„± ê°œì„ 

ì „ë¬¸ì„±: ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€

## ë‹µë³€ í¬ë§· ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)

âœ… í•„ìˆ˜ í¬ë§·:

1. **ì œëª© (1ì¤„)**
2. **ë¹ˆ ì¤„**
3. **ë³¸ë¬¸ (ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸)**
ê° ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.
ê° ë²ˆí˜¸ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë°˜ë“œì‹œ ì¶”ê°€í•˜ì„¸ìš”.
4. **ì°¸ê³  ë¬¸ì„œ ì„¹ì…˜**

ğŸ“š ì°¸ê³  ë¬¸ì„œ:

- ë¬¸ì„œëª… > (ì„¹ì…˜ëª…)
URL: https://...

í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?
"""

TABLE_MODE_PROMPT = """
ğŸš¨ í‘œ í˜•ì‹ ë‹µë³€ ëª¨ë“œ í™œì„±í™” - ì ˆëŒ€ ì¤€ìˆ˜ ğŸš¨

**ì‚¬ìš©ìê°€ í‘œ ëª¨ë“œë¥¼ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:**

1. ë‹µë³€ì˜ ì²« ì¤„ì€ ì œëª©ë§Œ ì‘ì„±
2. ì œëª© ë‹¤ìŒ ì¤„ë¶€í„° ì¦‰ì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ ì‹œì‘
3. ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3.)ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€

| í•­ëª© | ì„¤ëª… |
|------|------|
| ê°’1 | ë‚´ìš©1 |
"""

USER_MESSAGE_TEMPLATE = """ì•„ë˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘

{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘

{query}

ã€ë‹µë³€ ì‘ì„± ì§€ì¹¨ - ë§¤ìš° ì¤‘ìš”!ã€‘

1. ë°˜ë“œì‹œ ìœ„ì˜ "ë‹µë³€ í¬ë§· ê·œì¹™"ì„ ë”°ë¼ ì‘ì„±í•˜ì„¸ìš”
2. ì œëª©ì€ í•œ ì¤„ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”
3. ì œëª© ë‹¤ìŒì—ëŠ” ë°˜ë“œì‹œ ë¹ˆ ì¤„(ê°œí–‰)ì„ ì¶”ê°€í•˜ì„¸ìš”
4. ë³¸ë¬¸ì€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3., ...)ë¡œ êµ¬ì„±í•˜ì„¸ìš”
5. ê° ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”"""

TABLE_USER_MESSAGE_TEMPLATE = """ì•„ë˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘

{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘

{query}

ã€â€¼ï¸ í‘œ í˜•ì‹ ë‹µë³€ í•„ìˆ˜ã€‘

ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

# ===== LangChain 1.0 RAG ì„œë¹„ìŠ¤ (âœ… ë©”ì‹œì§€ ì €ì¥ ì œê±° - ë°˜í™˜ë§Œ) =====

class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (ë² ë”” í”„ë¡¬í”„íŠ¸ ì ìš©)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        print("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. LLM (ê°œì„ ëœ ì„¤ì •)
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.base_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", USER_MESSAGE_TEMPLATE)
        ])

        self.table_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT + TABLE_MODE_PROMPT),
            ("user", TABLE_USER_MESSAGE_TEMPLATE)
        ])

        print("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ")

    def _normalize_response(self, response: str) -> str:
        """âœ… ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ê·œí™” (ìëª¨ ë¶„ë¦¬ ë³µêµ¬)"""
        import re
        from unicodedata import normalize as unicode_normalize

        # 1. âœ… ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (ê°€ì¥ ì¤‘ìš”!)
        text = unicode_normalize('NFC', response)

        # 2. ì¤„ë°”ê¿ˆ í†µì¼
        text = text.replace('\r\n', '\n')
        text = text.replace('\r', '\n')

        # 3. 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        text = re.sub(r'\n{3,}', '\n\n', text)

        # 4. ê° ì¤„ ê³µë°± ì •ë¦¬
        lines = []
        for line in text.split('\n'):
            stripped = line.rstrip()
            stripped = re.sub(r' +', ' ', stripped)
            lines.append(stripped)
        text = '\n'.join(lines)

        # 5. ìµœì¢… ì •ë¦¬
        return text.strip()

    def process_query(
            self,
            user_id: str,
            query: str,
            table_mode: bool = False,
            supabase_client: Optional[SupabaseService] = None
    ) -> Dict[str, Any]:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬ (ì¼ë°˜ ì‘ë‹µ) - âœ… ì €ì¥ ì œê±°, ë°˜í™˜ë§Œ"""
        try:
            # âœ… í´ë¼ì´ì–¸íŠ¸ ì„ íƒ: ì „ë‹¬ëœ ê²ƒì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸€ë¡œë²Œ ì‚¬ìš©
            client = supabase_client if supabase_client else supabase_service

            # 1. Retriever ìƒì„± (ì‚¬ìš©ì í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
            retriever = SupabaseRetriever(
                embeddings=self.embeddings,
                supabase_client=client,
                k=5,
                threshold=0.3
            )

            # 2. ë¬¸ì„œ ê²€ìƒ‰
            context_text, raw_chunks = retriever.search(query)

            # 3. í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_template = self.table_prompt_template if table_mode else self.base_prompt_template

            # 4. ë©”ì‹œì§€ ìƒì„±
            messages = prompt_template.format_messages(
                context=context_text,
                query=query
            )

            # 5. LLM í˜¸ì¶œ
            response = self.llm.invoke(messages)
            ai_response = response.content

            # âœ… 6. ì‘ë‹µ ì •ê·œí™”
            ai_response = self._normalize_response(ai_response)

            # 7. ì†ŒìŠ¤ ID ì¶”ì¶œ
            source_chunk_ids = [
                chunk.get('id') for chunk in raw_chunks
                if chunk.get('id')
            ]

            # âŒ ë©”ì‹œì§€ ì €ì¥ ì œê±°! (ë¼ìš°í„°ì—ì„œ ì €ì¥)

            return {
                "user_query": query,
                "ai_response": ai_response,
                "source_chunks": raw_chunks,
                "source_chunk_ids": source_chunk_ids,
                "usage": {}
            }

        except Exception as e:
            print(f"âŒ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def process_query_streaming(
            self,
            user_id: str,
            query: str,
            table_mode: bool = False,
            supabase_client: Optional[SupabaseService] = None
    ) -> Generator[str, None, None]:
        """RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ - âœ… ì €ì¥ ì œê±°, ë°˜í™˜ë§Œ"""
        try:
            # âœ… í´ë¼ì´ì–¸íŠ¸ ì„ íƒ
            client = supabase_client if supabase_client else supabase_service

            # 1. Retriever ìƒì„±
            retriever = SupabaseRetriever(
                embeddings=self.embeddings,
                supabase_client=client,
                k=5,
                threshold=0.3
            )

            # 2. ë¬¸ì„œ ê²€ìƒ‰
            context_text, raw_chunks = retriever.search(query)

            # 3. í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_template = self.table_prompt_template if table_mode else self.base_prompt_template

            print(f"[RAG] table_mode: {table_mode}")
            if table_mode:
                print(f"[RAG] í‘œ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì¤‘")

            # 4. ë©”ì‹œì§€ ìƒì„±
            messages = prompt_template.format_messages(
                context=context_text,
                query=query
            )

            # 5. ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = chunk.content
                    # âœ… ê° í† í° ì •ê·œí™”
                    normalized_token = unicode_normalize('NFC', token)
                    yield normalized_token

            # âŒ ë©”ì‹œì§€ ì €ì¥ ì œê±°! (ë¼ìš°í„°ì—ì„œ ì €ì¥)

        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ]\n{str(e)}"

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
