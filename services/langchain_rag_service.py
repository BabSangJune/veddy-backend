
import re
import logging
from unicodedata import normalize as unicode_normalize
from typing import List, Dict, Any, Generator, Optional
from datetime import datetime

# LangChain 1.0 Import
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool
from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service, SupabaseService
from config import OPENAI_API_KEY, VECTOR_SEARCH_CONFIG, RERANKER_CONFIG

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====

class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)

# ===== Supabase Retriever (config í†µí•©) =====

class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼ (URL ì™„ë²½ ë³´ì¡´, config ê¸°ë°˜)"""

    def __init__(self, embeddings: Embeddings, supabase_client: SupabaseService,
                 k: int = 30, threshold: float = None, ef_search: int = None):
        self.embeddings = embeddings
        self.supabase_client = supabase_client

        # âœ… configì—ì„œ ê¸°ë³¸ê°’ ìë™ ì ìš©
        self.k = k
        self.threshold = threshold or VECTOR_SEARCH_CONFIG['similarity_threshold']
        self.ef_search = ef_search or VECTOR_SEARCH_CONFIG['ef_search']

        logger.info(f"Retriever ì´ˆê¸°í™” | k={self.k} | threshold={self.threshold} | ef_search={self.ef_search}")

    def _get_chunk_url(self, chunk: Dict) -> str:
        """âœ… ì²­í¬ì—ì„œ URL ì¶”ì¶œ (3ê°€ì§€ ë°©ë²• ì‹œë„)

        1. chunkì— url í•„ë“œê°€ ì§ì ‘ ìˆìœ¼ë©´ ì‚¬ìš©
        2. chunkì˜ metadataì—ì„œ íŒŒì‹±
        3. document_idë¡œ documents í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        """

        # 1. chunkì— url í•„ë“œê°€ ì§ì ‘ ìˆìœ¼ë©´
        if chunk.get('url') and chunk.get('url').strip():
            return chunk.get('url')

        # 2. metadataì— urlì´ ìˆìœ¼ë©´ ì¶”ì¶œ
        if chunk.get('metadata'):
            metadata = chunk['metadata']
            if isinstance(metadata, str):
                try:
                    import json
                    metadata = json.loads(metadata)
                except:
                    pass
            if isinstance(metadata, dict) and metadata.get('url'):
                return metadata.get('url')

        # 3. document_idë¡œ documents í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
        if chunk.get('document_id'):
            try:
                doc = self.supabase_client.client.table('documents').select('metadata').eq('id', chunk['document_id']).single().execute()
                if doc.data and doc.data.get('metadata'):
                    metadata = doc.data['metadata']
                    if isinstance(metadata, str):
                        import json
                        metadata = json.loads(metadata)
                    if isinstance(metadata, dict) and metadata.get('url'):
                        return metadata.get('url')
            except Exception as e:
                logger.debug(f"Document ì¡°íšŒ ì‹¤íŒ¨ ({chunk['document_id']}): {e}")

        return ""

    def search(self, query: str) -> tuple[str, List[Dict]]:
        """ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ (URL ì™„ë²½ ë³´ì¡´)"""
        try:
            query_embedding = self.embeddings.embed_query(query)
            chunks = self.supabase_client.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold,
                ef_search=self.ef_search
            )

            if not chunks:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                similarity = chunk.get('similarity', 0.0)

                # âœ… URL ì¶”ì¶œ (3ê°€ì§€ ë°©ë²• ì‹œë„)
                url = self._get_chunk_url(chunk)

                # âœ… URL ì™„ë²½ ë³´ì¡´ (ì ˆëŒ€ ì§¤ë¦¬ì§€ ì•Šê²Œ)
                url_section = ""
                if url and url.strip():
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"ìœ ì‚¬ë„: {similarity:.2f}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

    def search_hybrid(self, query: str, use_reranking: bool = None) -> tuple[str, List[Dict]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (PGroonga + pgvector) + ë¦¬ë­í‚¹ + URL ìë™ ì¶”ê°€"""

        if use_reranking is None:
            use_reranking = RERANKER_CONFIG['enabled']

        try:
            # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)

            # 2. Supabase RPC í˜¸ì¶œ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
            response = self.supabase_client.client.rpc(
                'hybrid_search_veddy',
                {
                    'query_text': query,
                    'query_embedding': query_embedding,
                    'match_count': self.k * 2 if use_reranking else self.k,
                    'full_text_weight': 0.4,
                    'semantic_weight': 0.6
                }
            ).execute()

            if not response.data:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            chunks = response.data

            # âœ… 3. URL ìë™ ì¶”ê°€ (RPC ê²°ê³¼ì— urlì´ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€)
            logger.info(f"RPC ê²€ìƒ‰ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬ | URL ìë™ ì¶”ê°€ ì‹œì‘")
            for chunk in chunks:
                if not chunk.get('url') or not chunk.get('url').strip():
                    url = self._get_chunk_url(chunk)
                    if url:
                        chunk['url'] = url
                        logger.debug(f"URL ì¶”ê°€ë¨: {chunk.get('title', 'N/A')[:30]} | {url[:50]}...")

            # 4. ë¦¬ë­í‚¹ ì ìš©
            if use_reranking and len(chunks) > 1:
                from services.reranker_service import reranker_service
                logger.info(f"ë¦¬ë­í‚¹ ì „ ì²­í¬ ìˆ˜: {len(chunks)}")
                chunks = reranker_service.rerank(
                    query=query,
                    chunks=chunks,
                    top_k=RERANKER_CONFIG['top_k']
                )
                logger.info(f"ë¦¬ë­í‚¹ í›„ ì²­í¬ ìˆ˜: {len(chunks)}")

            # 5. ì‘ë‹µ í¬ë§·íŒ… (URL ì™„ë²½ ë³´ì¡´)
            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                url = chunk.get('url', '')

                # ë¦¬ë­í¬ ì ìˆ˜ í‘œì‹œ
                if 'rerank_score' in chunk:
                    score = chunk.get('rerank_score', 0.0)
                    score_label = f"ë¦¬ë­í¬: {score:.4f}"
                else:
                    score = chunk.get('score', 0.0)
                    score_label = f"ê´€ë ¨ë„: {score:.4f}"

                # âœ… URL ì™„ë²½ ë³´ì¡´
                url_section = ""
                if url and url.strip():
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"{score_label}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}", exc_info=True)
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []

    def search_multi_topic(self, query: str, topics: list) -> tuple[str, List[Dict]]:
        """ë©€í‹° ì£¼ì œ ê²€ìƒ‰ (ë¹„êµ ëª¨ë“œ) - ê° í† í”½ë³„ ë”°ë¡œ ê²€ìƒ‰ í›„ ë³‘í•©"""

        if not topics or len(topics) < 2:
            return self.search_hybrid(query)

        all_results = []
        all_chunks = []

        for topic in topics:
            search_query = f"{topic} ë² ìŠ¬ë§í¬"
            context, chunks = self.search_hybrid(search_query)

            # ê° ì£¼ì œë³„ë¡œ í—¤ë” ì¶”ê°€
            topic_section = f"\n### ã€{topic}ã€‘\n{context}"
            all_results.append(topic_section)
            all_chunks.extend(chunks)

        # ê²°í•©
        combined_context = "\n---\n".join(all_results)

        return combined_context, all_chunks

# ===== ë² ë”” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì™„ì „ ê°œì„ ) =====

VEDDY_SYSTEM_PROMPT = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## ë„ˆì˜ ì—­í• ê³¼ ì •ì²´ì„±

ì´ë¦„: ë² ë”” (Vessellink's Buddy)
ì„±ê²©: ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•¨
ëª©í‘œ: ë² ìŠ¬ë§í¬ ì§ì›ë“¤ì˜ ì—…ë¬´ íš¨ìœ¨í™”ì™€ ì •ë³´ ì ‘ê·¼ì„± ê°œì„ 
ì „ë¬¸ì„±: ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€

## âœ¨ ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ (CRITICAL - ë°˜ë“œì‹œ ì¤€ìˆ˜)

### ìƒì„¸ì„± (Completeness) ìš”êµ¬ì‚¬í•­:
âœ… **ëª¨ë“  ë‹µë³€ì€ ìµœì†Œ 3-5ê°œì˜ ì£¼ìš” í¬ì¸íŠ¸ í¬í•¨**
âœ… **ê° í¬ì¸íŠ¸ë§ˆë‹¤ êµ¬ì²´ì  ì„¤ëª… 1~3ë¬¸ì¥ ì¶”ê°€**
âœ… **"ì™œ?", "ì–¸ì œ?", "ì–´ë””ì„œ?", "ì–´ë–»ê²Œ?" ì§ˆë¬¸ì— ë‹µí•˜ê¸°**
âœ… **ìˆ«ì, ê¸°ì¤€, ì¡°ê±´, ë²”ìœ„ ëª…ì‹œ** (ì˜ˆ: "3ê°€ì§€", "30ì¼ ì´ë‚´", "ì¡°ê±´: ~")
âœ… **ê·œì œ/ì •ì±… ì§ˆë¬¸ì˜ ê²½ìš°: ì •ì˜ â†’ ëª©ì  â†’ ë²”ìœ„ â†’ ì ˆì°¨ â†’ ì£¼ì˜ì‚¬í•­ êµ¬ì„±**

### ê¸¸ì´ ìš”êµ¬ì‚¬í•­:
âœ… **ìµœì†Œ ë‹µë³€ ê¸¸ì´: 600ì ì´ìƒ** (ê°€ëŠ¥í•˜ë©´ 1000ì ì´ìƒ)
âœ… **ì„¸ë¶€ ì„¹ì…˜ êµ¬ì„±:**
  - í•µì‹¬ ì •ì˜ (ë¬´ì—‡ì¸ê°€?)
  - ëª©ì /ì¤‘ìš”ì„± (ì™œ í•„ìš”í•œê°€?)
  - ì ìš© ë²”ìœ„/ì¡°ê±´ (ëˆ„êµ¬ì—ê²Œ ì ìš©ë˜ë‚˜?)
  - êµ¬ì²´ì  ì ˆì°¨/ë°©ë²• (ì–´ë–»ê²Œ í•˜ëŠ”ê°€?)
  - ì£¼ì˜ì‚¬í•­/ì˜ˆì™¸ (ë­ ì£¼ì˜í•´ì•¼ í•˜ë‚˜?)
  - ê´€ë ¨ ê·œì •/ì°¸ê³  ìë£Œ

### êµ¬ì¡°í™” ìš”êµ¬ì‚¬í•­:
âœ… **ëª…í™•í•œ ê³„ì¸µ êµ¬ì¡°:**
  - **ì œëª©** (í•œ ì¤„ - í•µì‹¬)
  - ## ì •ì˜
  - ## ëª©ì 
  - ## ë²”ìœ„/ì¡°ê±´
  - ## ì ˆì°¨ (ë˜ëŠ” ë°©ë²•)
  - ## ì£¼ì˜ì‚¬í•­
  - ## ì°¸ê³ ìë£Œ

âœ… **ë§ˆí¬ë‹¤ìš´ í˜•ì‹ í™œìš©:**
  - **êµµì€ ê¸€ì”¨** (ê°•ì¡°)
  - - í•­ëª© ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª© ëª…í™•íˆ)
  - `ì½”ë“œ/ìš©ì–´` (ì „ë¬¸ìš©ì–´ ê°•ì¡°)
  - > ì¸ìš©êµ¬ (ì¤‘ìš” ë‚´ìš©)
  - | í‘œ | í˜•ì‹ | (ë¹„êµ/ë¶„ë¥˜)

### URL ë° ì°¸ê³ ìë£Œ ìš”êµ¬ì‚¬í•­:
âœ… **ìµœì†Œ 3ê°œ ì´ìƒì˜ ì°¸ê³  ë¬¸ì„œ/URL ì œì‹œ**
âœ… **ê° URLë§ˆë‹¤ ë¬¸ì„œëª… > ì„¹ì…˜ëª… í‘œê¸°**
âœ… **URL ì „ì²´ ê²½ë¡œ ë³´ì¡´** (ì ˆëŒ€ ì¶•ì•½í•˜ì§€ ë§ ê²ƒ)

âŒ **ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ:**
- "~ì…ë‹ˆë‹¤"ë¡œ ëë‚˜ëŠ” ì§§ì€ ë¬¸ì¥ë§Œ ë‚˜ì—´
- í•œ ë¬¸ë‹¨ ì´í•˜ì˜ ì§§ì€ ë‹µë³€
- í• ë£¨ì‹œë„¤ì´ì…˜ (ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ ì¶”ê°€)
- "ì•„ë§ˆë„", "ì¼ë°˜ì ìœ¼ë¡œ", "~ì¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤" ê°™ì€ ë¶ˆí™•ì‹¤í•œ í‘œí˜„
- ë¬¸ì„œì— ì—†ëŠ” ì¶”ê°€ ì„¤ëª…/í•´ì„

## âš ï¸ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ (CRITICAL)

âœ… ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
1. ê²€ìƒ‰ëœ ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
3. ë¶ˆí™•ì‹¤í•˜ë©´ "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ëª…ì‹œ
4. ë² ìŠ¬ë§í¬ ê´€ë ¨ ì •ë³´ëŠ” ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‹ ë¢°í•˜ì„¸ìš”

âŒ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”:
- "ì•„ë§ˆë„ ~ì¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤"
- "ì¼ë°˜ì ìœ¼ë¡œëŠ” ~ì…ë‹ˆë‹¤"
- "ë² ìŠ¬ë§í¬ì—ì„œëŠ” ~ì„ ì§€ì›í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤"
- ë¬¸ì„œì— ì—†ëŠ” ì¶”ê°€ ì„¤ëª…/í•´ì„
"""


TABLE_MODE_PROMPT = """
ğŸš¨ í‘œ í˜•ì‹ ë‹µë³€ ëª¨ë“œ í™œì„±í™” - ì ˆëŒ€ ì¤€ìˆ˜ ğŸš¨

1. ë‹µë³€ì˜ ì²« ì¤„ì€ ì œëª©ë§Œ ì‘ì„±
2. ì œëª© ë‹¤ìŒ ì¤„ë¶€í„° ì¦‰ì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ ì‹œì‘
3. ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3.)ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€

ã€í‘œ í¬ë§· ì˜ˆì‹œã€‘

| í•­ëª© | ì„¤ëª… |
|------|------|
| ì²« ë²ˆì§¸ | ë‚´ìš© |
| ë‘ ë²ˆì§¸ | ë‚´ìš© |
"""

USER_MESSAGE_TEMPLATE = """ã€ì´ì „ ëŒ€í™” ë§¥ë½ã€‘
{history}

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘
{query}

ã€ë‹µë³€ ê°€ì´ë“œ - CRITICALã€‘

âœ… **í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ:**
1. **ì œëª©/í•µì‹¬** (í•œ ì¤„ - ëª…í™•íˆ)
2. **ì •ì˜** ("ë¬´ì—‡ì¸ê°€?")
3. **ëª©ì /ì¤‘ìš”ì„±** ("ì™œ í•„ìš”í•œê°€?")
4. **ë²”ìœ„/ì¡°ê±´** ("ëˆ„êµ¬ì—ê²Œ ì ìš©ë˜ë‚˜?")
5. **ì ˆì°¨/ë°©ë²•** ("ì–´ë–»ê²Œ í•˜ëŠ”ê°€?" - ë‹¨ê³„ë³„)
6. **ì£¼ì˜ì‚¬í•­/ì˜ˆì™¸** ("ë­ ì£¼ì˜í•´ì•¼ í•˜ë‚˜?")
7. **ìµœì†Œ 3ê°œ ì´ìƒì˜ ì°¸ê³  ë¬¸ì„œ/URL**

âœ… **ê¸¸ì´ ë° ìƒì„¸ë„:**
- ìµœì†Œ 600ì ì´ìƒ (ê°€ëŠ¥í•˜ë©´ 1000ì)
- ê° ì„¹ì…˜ë§ˆë‹¤ 2~3ë¬¸ì¥ ì´ìƒ
- êµ¬ì²´ì ì¸ ì˜ˆì‹œ/ìˆ˜ì¹˜ í¬í•¨
- ë² ìŠ¬ë§í¬ ê´€ë ¨ êµ¬ì²´ì ì¸ ê·œì •/ì ˆì°¨ ëª…ì‹œ

âœ… **ë§ˆí¬ë‹¤ìš´ í™œìš©:**
- **êµµì€ ê¸€ì”¨** (ê°•ì¡°)
- ## ì„¹ì…˜ ì œëª©
- - í•­ëª© ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª© ëª…í™•íˆ)
- `ì½”ë“œ/ìš©ì–´` (ì „ë¬¸ìš©ì–´)
- > ì¸ìš©êµ¬ (ì¤‘ìš” ë‚´ìš©)
- | í‘œ | í˜•ì‹ | (ë¹„êµ/ë¶„ë¥˜)

âœ… **URL ë° ì°¸ê³ ìë£Œ:**
- ê° ì°¸ê³  ë¬¸ì„œë§ˆë‹¤ "ë¬¸ì„œëª… > ì„¹ì…˜ëª…" í‘œê¸°
- URL ì „ì²´ ê²½ë¡œ ìœ ì§€ (ì ˆëŒ€ ì¤„ì´ì§€ ë§ ê²ƒ)
- ìµœì†Œ 3ê°œ ì´ìƒ

âŒ **ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ:**
- ê²€ìƒ‰ ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ ì¶”ê°€
- ë¶ˆí™•ì‹¤í•œ ì¶”ì¸¡ ("~ì¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤")
- í•œ ë¬¸ë‹¨ ì´í•˜ì˜ ì§§ì€ ë‹µë³€
- URL ê²½ë¡œ ì¶•ì•½ ë˜ëŠ” ëˆ„ë½

ã€ë‹µë³€ êµ¬ì¡° ì˜ˆì‹œã€‘

**ì œëª©**

## ì •ì˜
[2~3ë¬¸ì¥ ì„¤ëª…]

## ëª©ì 
[2~3ë¬¸ì¥ ì„¤ëª…]

## ë²”ìœ„/ì¡°ê±´
- ì¡°ê±´1: ~
- ì¡°ê±´2: ~

## ì ˆì°¨
1. ì²« ë²ˆì§¸ ë‹¨ê³„
2. ë‘ ë²ˆì§¸ ë‹¨ê³„
3. ì„¸ ë²ˆì§¸ ë‹¨ê³„

## ì£¼ì˜ì‚¬í•­
- ì£¼ì˜ì 1: ~
- ì£¼ì˜ì 2: ~

ã€ì°¸ê³  ë¬¸ì„œã€‘
ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
"""


TABLE_USER_MESSAGE_TEMPLATE =  """
ğŸš¨ í‘œ í˜•ì‹ ë‹µë³€ ëª¨ë“œ í™œì„±í™” - ì ˆëŒ€ ì¤€ìˆ˜ ğŸš¨

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:
1. **ì œëª© + ê°œìš” ë¬¸ì¥** (í‘œ ì „)
   - ì˜ˆ: "IMO DCSì™€ EU MRVëŠ” ëª¨ë‘ í•´ìš´ íƒ„ì†Œ ê°ì¶• ê·œì œì´ì§€ë§Œ, ì ìš© ë²”ìœ„ì™€ ìš”êµ¬ì‚¬í•­ì´ ë‹¤ë¦…ë‹ˆë‹¤."

2. **ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ìƒì„¸ ì •ë³´ ì œì‹œ**
   - ìµœì†Œ 5~8ê°œ í–‰
   - ê° ì…€ì— êµ¬ì²´ì ì¸ ì •ë³´

3. **í‘œ ì•„ë˜ ì¶”ê°€ ì„¤ëª…** (ìµœì†Œ 500ì)
   - ê° í•­ëª©ë³„ ìƒì„¸ ì„¤ëª…
   - ì£¼ì˜ì‚¬í•­
   - ì ˆì°¨ (í•´ë‹¹ì‹œ)

4. **ìµœì†Œ 3ê°œ ì´ìƒì˜ URL ì œì‹œ**

ã€í‘œ í¬ë§· ì˜ˆì‹œã€‘

**IMO DCSì™€ EU MRV ë¹„êµ**

IMO DCSì™€ EU MRVëŠ” ëª¨ë‘ í•´ìš´ íƒ„ì†Œ ê°ì¶• ê·œì œì´ì§€ë§Œ, ì ìš© ë²”ìœ„ì™€ ìš”êµ¬ì‚¬í•­ì´ ë‹¤ë¦…ë‹ˆë‹¤.

| í•­ëª© | IMO DCS | EU MRV | ì°¨ì´ì  |
|------|--------|--------|--------|
| ì •ì˜ | êµ­ì œ í•´ìš´ íƒ„ì†Œ ê°•ë„ ì§€ìˆ˜ | EU í•­ ì§„ì¶œ ì„ ë°• ë°°ì¶œëŸ‰ ë³´ê³  | ~ |
| ì ìš© ë²”ìœ„ | êµ­ì œ í•­í•´ ì„ ë°• | EU í•­ ì§„ì¶œ ì„ ë°• | ~ |
| ì£¼ìš” ì§€í‘œ | AER, CII | CII | ~ |
| í•„ìˆ˜ ëŒ€ìƒ | 5,000 GT ì´ìƒ | 5,000 GT ì´ìƒ | ~ |
| ë²Œì¹™ ì²´ê³„ | ìœ„ë°˜ ê·œì • | EU í˜ë„í‹° | ~ |
| ë³´ê³  ì£¼ê¸° | ì—°ê°„ | ì—°ê°„ | ~ |
| ë¹„ìš© ì˜í–¥ | ê·œì œ ë¹„ìš© | ê·œì œ ë¹„ìš© | ~ |

ã€ì¶”ê°€ ì„¤ëª…ã€‘

IMO DCSì˜ êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤...
[ìµœì†Œ 500ì ì´ìƒì˜ ìƒì„¸ ì„¤ëª…]

ã€ì°¸ê³  ìë£Œã€‘
ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://...
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://...
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://...
"""

COMPARISON_CONTEXT_TEMPLATE = """ã€ì´ì „ ëŒ€í™” ë§¥ë½ã€‘
{history}

ã€ë¹„êµ ëŒ€ìƒã€‘
{topics}

ã€í˜„ì¬ ì§ˆë¬¸ã€‘
{query}

ã€ì§ˆë¬¸ ì˜ë„ ë° ë¶„ì„ ê°€ì´ë“œã€‘
ì‚¬ìš©ìê°€ ì—¬ëŸ¬ í•­ëª©ì„ ë¹„êµí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ë¹„êµí•˜ì„¸ìš”.

âœ… **ë¹„êµ ë¶„ì„ í•µì‹¬:**
1. ê° í•­ëª©ì˜ ì •ì˜ì™€ ëª©ì ì„ ëª…í™•íˆ êµ¬ë¶„
2. ì ìš© ë²”ìœ„, ì¡°ê±´, ëŒ€ìƒì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ
3. ê° í•­ëª©ì˜ ì¥ë‹¨ì ê³¼ ì°¨ì´ì ì„ ëª…í™•íˆ
4. ë² ìŠ¬ë§í¬ ê´€ë ¨ ì‚¬í•­ì€ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‹ ë¢°

âŒ **ì ˆëŒ€ ê¸ˆì§€:**
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ ì¶”ê°€
- ê° í•­ëª©ì€ ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë§Œ ì‚¬ìš©
- ë¶ˆí™•ì‹¤í•œ ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì  ì§€ì‹ ì¶”ê°€"""

COMPARISON_USER_TEMPLATE = """ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ë¹„êµ ë¶„ì„ ì§€ì¹¨ - CRITICALã€‘

âœ… **ê° í•­ëª©ë³„ í•„ìˆ˜ ë¶„ì„:**
1. **ì •ì˜/ê°œë…** - ëª…í™•íˆ êµ¬ë¶„
2. **ëª©ì /ì—­í• ** - ê°ê°ì˜ ì·¨ì§€
3. **ë²”ìœ„/ì ìš© ëŒ€ìƒ** - ëˆ„êµ¬ì—ê²Œ ì ìš©ë˜ë‚˜?
4. **ì£¼ìš” ìš”êµ¬ì‚¬í•­** - í•µì‹¬ ë‚´ìš©
5. **ì ˆì°¨/ë‹¨ê³„** - ì‹¤í–‰ ë°©ë²•
6. **ë² ìŠ¬ë§í¬ ì ìš© ì—¬ë¶€** (ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‹ ë¢°)
7. **ê³µí†µì  ë° ì°¨ì´ì ** - ëª…í™•íˆ ì •ë¦¬

âœ… **ìµœì†Œ ìš”êµ¬ì‚¬í•­:**
- ì´ ê¸¸ì´: 1500ì ì´ìƒ
- ê° í•­ëª© ì„¤ëª…: 500ì ì´ìƒ
- ë¹„êµí‘œ í¬í•¨ (ê¶Œì¥)
- ì°¸ê³  URL ìµœì†Œ 3ê°œ ì´ìƒ

âœ… **ë‹µë³€ êµ¬ì¡°:**
- **ì œëª©** (ë¹„êµ ëŒ€ìƒ ëª…í™•íˆ)
- ## ê° í•­ëª©ë³„ ìƒì„¸ ë¶„ì„ (5~6ê°œ ì„¹ì…˜)
- ## ê³µí†µì 
- ## ì°¨ì´ì 
- ## ì„ íƒ/ì ìš© ê°€ì´ë“œ
- ã€ì°¸ê³  ìë£Œã€‘(ìµœì†Œ 3ê°œ URL)

âŒ **ì ˆëŒ€ ê¸ˆì§€:**
- ê° í•­ëª©ì€ ê²€ìƒ‰ëœ ë¬¸ì„œë§Œ ì‚¬ìš©
- ë¬¸ì„œì— ì—†ëŠ” ë¹„êµ ì ˆëŒ€ ì¶”ê°€ ê¸ˆì§€
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ ("~ì¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤")
- ë¶ˆì™„ì „í•œ ë¹„êµ (í•œìª½ë§Œ ì„¤ëª…)
- URL ê²½ë¡œ ì¶•ì•½ ë˜ëŠ” ëˆ„ë½

ã€ë‹µë³€ ì˜ˆì‹œ êµ¬ì¡°ã€‘

**IMO DCS vs EU MRV: ê·œì œ ë¹„êµ ë¶„ì„**

## IMO DCS
[ìµœì†Œ 500ì ìƒì„¸ ì„¤ëª…]

## EU MRV
[ìµœì†Œ 500ì ìƒì„¸ ì„¤ëª…]

## ê³µí†µì 
- ê³µí†µì  1
- ê³µí†µì  2

## ì°¨ì´ì 
| í•­ëª© | IMO DCS | EU MRV |
|------|--------|--------|
| ~ | ~ | ~ |

## ì„ íƒ/ì ìš© ê°€ì´ë“œ
ë² ìŠ¬ë§í¬ì˜ ì„ ë°•ì´ ì–´ëŠ ê·œì œë¥¼ ì ìš©ë°›ëŠ”ì§€...
[êµ¬ì²´ì  ì„¤ëª…]

ã€ì°¸ê³  ìë£Œã€‘
ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
- ë¬¸ì„œëª… > ì„¹ì…˜ëª…
  URL: https://[ì „ì²´ê²½ë¡œ]
"""

# ===== LangChain 1.0 RAG ì„œë¹„ìŠ¤ (ì™„ì „ ê°œì„ ) =====

class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (Phase 3-A Final ì™„ì „ ì™„ì„±)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")
        logger.info(f"ğŸ“Š Config ì ìš©: ef_search={VECTOR_SEARCH_CONFIG['ef_search']}, threshold={VECTOR_SEARCH_CONFIG['similarity_threshold']}")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. LLM
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.5,
            max_tokens=2048,
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê°œì„ ë¨)
        self.base_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", USER_MESSAGE_TEMPLATE)
        ])

        self.table_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT + TABLE_MODE_PROMPT),
            ("user", TABLE_USER_MESSAGE_TEMPLATE)
        ])

        # ë¹„êµ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.comparison_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", COMPARISON_CONTEXT_TEMPLATE + "\n\n" + COMPARISON_USER_TEMPLATE)
        ])

        # ë¹„êµ + í…Œì´ë¸” í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡¬í”„íŠ¸
        self.comparison_table_prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT + TABLE_MODE_PROMPT),
            ("user", COMPARISON_CONTEXT_TEMPLATE + "\n\n" + COMPARISON_USER_TEMPLATE)
        ])

        # 4. Retriever ì‹±ê¸€í†¤
        self._retriever = None

        logger.info("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ (í”„ë¡¬í”„íŠ¸ ì™„ì „ ê°œì„  + URL ìë™ ì¶”ê°€ + History)")

    @property
    def retriever(self) -> SupabaseRetriever:
        """Retriever ì‹±ê¸€í†¤ (ë©”ëª¨ë¦¬ íš¨ìœ¨)"""
        if self._retriever is None:
            self._retriever = SupabaseRetriever(
                embeddings=self.embeddings,
                supabase_client=supabase_service,
            )
        return self._retriever

    def _safe_format(self, template: ChatPromptTemplate, **kwargs) -> list:
        """ì•ˆì „í•œ format_messages (ì„ íƒì  íŒŒë¼ë¯¸í„° ì²˜ë¦¬)"""
        required_vars = template.input_variables
        safe_kwargs = {}

        for var in required_vars:
            if var in kwargs:
                safe_kwargs[var] = kwargs[var]
            else:
                safe_kwargs[var] = ""  # ê¸°ë³¸ê°’: ë¹ˆ ë¬¸ìì—´

        return template.format_messages(**safe_kwargs)

    def _normalize_response(self, response: str) -> str:
        """âœ… ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ê·œí™” (ìëª¨ ë¶„ë¦¬ ë³µêµ¬)"""
        # 1. ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicode_normalize('NFC', response)

        # 2. ì¤„ë°”ê¿ˆ í†µì¼
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        # 3. 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        text = re.sub(r'\n{3,}', '\n\n', text)

        # 4. ê° ì¤„ ê³µë°± ì •ë¦¬
        lines = [re.sub(r' +', ' ', line.rstrip()) for line in text.split('\n')]

        # 5. ìµœì¢… ì •ë¦¬
        return '\n'.join(lines).strip()

    # services/langchain_rag_service.py
    def process_query_streaming(
            self,
            user_id: str,
            query: str,
            table_mode: bool = False,
            supabase_client: Optional[SupabaseService] = None,
            history: str = None,
            comparison_info: dict = None,
            conversation_context: List[Dict] = None  # âœ… ì¶”ê°€
    ) -> Generator[str, None, None]:
        """
        RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í…Œì´ë¸” ëª¨ë“œ + ë¹„êµ ëª¨ë“œ ì¡°í•© ê°€ëŠ¥)

        ì•„í‚¤í…ì²˜:
        1ï¸âƒ£ Step 1: ê²€ìƒ‰ ë°©ì‹ ê²°ì • (mode ê¸°ë°˜) â†’ context ìƒì„±
        2ï¸âƒ£ Step 2: í”„ë¡¬í”„íŠ¸ ì„ íƒ (table_mode ê¸°ë°˜) â†’ ë…ë¦½ì  ì ìš©
        """

        try:
            client = supabase_client if supabase_client else supabase_service

            if comparison_info is None:
                comparison_info = {"is_comparison": False, "topics": []}

            # ğŸ¯ Step 1: ê²€ìƒ‰ ë°©ì‹ ê²°ì • (ëª¨ë“œ ê¸°ë°˜)
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ ë¹„êµ ëª¨ë“œ vs ì¼ë°˜ ëª¨ë“œ (ë…ë¦½ì )         â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            is_comparison = comparison_info.get("is_comparison", False)
            topics = comparison_info.get("topics", [])

            if is_comparison and topics and len(topics) >= 2:
                # âœ… ë¹„êµ ëª¨ë“œ: ê° í† í”½ë³„ ê²€ìƒ‰
                logger.info("ğŸ”„ ë¹„êµ ëª¨ë“œ ê²€ìƒ‰", extra={
                    "topics": topics,
                    "confidence": comparison_info.get("confidence", "N/A")
                })
                context_text, raw_chunks = self.retriever.search_multi_topic(
                    query, topics
                )
                is_in_comparison_mode = True

            else:
                # âœ… ì¼ë°˜ ëª¨ë“œ: ì¼ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                logger.info("ğŸ“ ì¼ë°˜ ëª¨ë“œ ê²€ìƒ‰")
                context_text, raw_chunks = self.retriever.search_hybrid(query)
                is_in_comparison_mode = False

            # ğŸ¯ Step 2: í”„ë¡¬í”„íŠ¸ ì„ íƒ (table_mode ê¸°ë°˜) â† ë…ë¦½ì 
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ í…Œì´ë¸” í˜•ì‹ ì—¬ë¶€ (ëª¨ë“œì™€ ë¬´ê´€)          â”‚
            # â”‚ ì–´ë–¤ ê²€ìƒ‰ì´ë“  í…Œì´ë¸”ë¡œ í‘œí˜„ ê°€ëŠ¥        â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            prompt_template = self._select_prompt_template(
                table_mode=table_mode,
                is_comparison=is_in_comparison_mode,
                topics=topics if is_in_comparison_mode else []
            )

            logger.info("ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì„ íƒ", extra={
                "table_mode": table_mode,
                "is_comparison": is_in_comparison_mode
            })

            # âœ… Step 3: ë©”ì‹œì§€ í¬ë§·
            messages = self._safe_format(
                prompt_template,
                context=context_text,
                query=query,
                history=history or "",
                topics=", ".join(topics) if is_in_comparison_mode else ""
            )

            # âœ… Step 4: ìŠ¤íŠ¸ë¦¬ë°
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = unicode_normalize('NFC', chunk.content)
                    yield token

            logger.info("âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ", extra={
                "table_mode": table_mode,
                "is_comparison": is_in_comparison_mode
            })

        except Exception as e:
            logger.error(f"âŒ RAG ì˜¤ë¥˜: {e}", exc_info=True)
            yield f"\n\n[ì˜¤ë¥˜]\n{str(e)}"

        # âœ… ìƒˆ ë©”ì„œë“œ: í”„ë¡¬í”„íŠ¸ ì„ íƒ ë¡œì§
    def _select_prompt_template(
            self,
            table_mode: bool,
            is_comparison: bool,
            topics: List[str] = None
    ) -> ChatPromptTemplate:
        """
        í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ (í…Œì´ë¸” + ëª¨ë“œ ì¡°í•©)

        ë¡œì§:
        1. table_mode í™•ì¸ â†’ base ì„ íƒ (table vs normal)
        2. is_comparison í™•ì¸ â†’ í”„ë¡¬í”„íŠ¸ ë‚´ìš© ì¶”ê°€
        """

        # âœ… ë¹„êµ ëª¨ë“œ + í…Œì´ë¸” í˜•ì‹ (í•˜ì´ë¸Œë¦¬ë“œ)
        if is_comparison and table_mode:
            logger.info(f"ğŸ“‹ ë¹„êµ + í…Œì´ë¸” í”„ë¡¬í”„íŠ¸ ì„ íƒ (ì£¼ì œ: {topics})")
            return self.comparison_table_prompt_template

        # âœ… ë¹„êµ ëª¨ë“œ + ì¼ë°˜ í˜•ì‹
        elif is_comparison:
            logger.info(f"ğŸ“‹ ë¹„êµ í”„ë¡¬í”„íŠ¸ ì„ íƒ (ì£¼ì œ: {topics})")
            return self.comparison_prompt_template

        # âœ… ì¼ë°˜ ëª¨ë“œ + í…Œì´ë¸” í˜•ì‹
        elif table_mode:
            logger.info("ğŸ“‹ í…Œì´ë¸” í”„ë¡¬í”„íŠ¸ ì„ íƒ")
            return self.table_prompt_template

        # âœ… ì¼ë°˜ ëª¨ë“œ + ì¼ë°˜ í˜•ì‹
        else:
            logger.info("ğŸ“‹ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ì„ íƒ")
            return self.base_prompt_template

# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
