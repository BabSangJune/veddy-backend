# services/langchain_rag_service.py (LangChain 1.0 + ë² ë”” í”„ë¡¬í”„íŠ¸ ê°œì„ )
import re
from unicodedata import normalize as unicode_normalize
from typing import List, Dict, Any, Generator

# LangChain 1.0 Import
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.tools import tool

from langchain.agents import create_agent

from services.embedding_service import embedding_service
from services.supabase_service import supabase_service
from config import OPENAI_API_KEY


# ===== ì»¤ìŠ¤í…€ ì„ë² ë”© ë˜í¼ =====
class CustomEmbeddings(Embeddings):
    """BGE-m3-koë¥¼ LangChain Embeddingsë¡œ ë˜í•‘"""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return embedding_service.embed_batch(texts)

    def embed_query(self, text: str) -> List[float]:
        return embedding_service.embed_text(text)


# ===== Supabase Retriever =====
class SupabaseRetriever:
    """Supabase ê²€ìƒ‰ ë˜í¼ (URL í¬í•¨)"""

    def __init__(self, embeddings: Embeddings, k: int = 5, threshold: float = 0.3):
        self.embeddings = embeddings
        self.k = k
        self.threshold = threshold

    def search(self, query: str) -> tuple[str, List[Dict]]:
        """
        ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰ (URL ì™„ë²½ ë³´ì¡´)
        """
        try:
            query_embedding = self.embeddings.embed_query(query)
            chunks = supabase_service.search_chunks(
                embedding=query_embedding,
                limit=self.k,
                threshold=self.threshold
            )

            if not chunks:
                return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

            context_parts = []
            for i, chunk in enumerate(chunks, 1):
                title = chunk.get('title', 'ì œëª© ì—†ìŒ')
                content = chunk.get('content', '')
                source = chunk.get('source', 'ì¶œì²˜ ë¯¸ìƒ')
                url = chunk.get('url', '')
                similarity = chunk.get('similarity', 0.0)

                # âœ… URL ë³´ì¡´ (ì ˆëŒ€ ì‚­ì œ ê¸ˆì§€)
                url_section = ""
                if url and url.strip():  # URLì´ ìˆê³  ê³µë°±ì´ ì•„ë‹ˆë©´
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}\nğŸ”— URL: {url}"
                else:
                    url_section = f"\nğŸ“ ì¶œì²˜: {source}"

                context_parts.append(
                    f"[ë¬¸ì„œ {i}] {title}\n"
                    f"ìœ ì‚¬ë„: {similarity:.2f}\n"
                    f"ë‚´ìš©:\n{content}{url_section}"
                )

            formatted_context = "\n\n---\n\n".join(context_parts)
            return formatted_context, chunks

        except Exception as e:
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", []



# ===== ë² ë”” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê°œì„ ) =====
# services/langchain_rag_service.py

# ===== ê°œì„ ëœ ë² ë”” í”„ë¡¬í”„íŠ¸ =====
VEDDY_SYSTEM_PROMPT = """ë„ˆëŠ” ë² ìŠ¬ë§í¬ì˜ ë‚´ë¶€ AI ì–´ì‹œìŠ¤í„´íŠ¸ 'ë² ë””(VEDDY)'ì•¼.

## ë„ˆì˜ ì—­í• ê³¼ ì •ì²´ì„±

ì´ë¦„: ë² ë”” (Vessellink's Buddy)
ì„±ê²©: ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¨ìˆœí•˜ê³  ì„±ì‹¤í•¨
ëª©í‘œ: ë² ìŠ¬ë§í¬ ì§ì›ë“¤ì˜ ì—…ë¬´ íš¨ìœ¨í™”ì™€ ì •ë³´ ì ‘ê·¼ì„± ê°œì„ 
ì „ë¬¸ì„±: ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ì— ê¸°ë°˜í•œ ì •í™•í•œ ë‹µë³€

## ë‹µë³€ í¬ë§· ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜)

âœ… í•„ìˆ˜ í¬ë§·:

** ì œì¼ ì¤‘ìš” **
ë‹µë³€ ì‹œ ë‹¤ìŒ Markdown ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:

1. ë¬¸ë‹¨ êµ¬ë¶„: ë¹ˆ ì¤„ 2ê°œ (\n\n)
2. ì œëª©: # ë˜ëŠ” ##, ### ì‚¬ìš©
3. ë¦¬ìŠ¤íŠ¸: - ë˜ëŠ” 1. ë¡œ ì‹œì‘
4. ê°•ì¡°: **êµµê²Œ** ë˜ëŠ” *ê¸°ìš¸ì„*

ê·¸ ì™¸
1. **ì œëª© (1ì¤„)**

2. **ë¹ˆ ì¤„**

3. **ë³¸ë¬¸ (ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸)**

ê° ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”.
ê° ë²ˆí˜¸ ì‚¬ì´ì—ëŠ” ë¹ˆ ì¤„ 1ì¤„ì„ ë°˜ë“œì‹œ ì¶”ê°€í•˜ì„¸ìš”.

4. **í•˜ìœ„ í•­ëª© (ë“¤ì—¬ì“°ê¸°)**

  - ì„¸ë¶€ ì‚¬í•­ 1
  - ì„¸ë¶€ ì‚¬í•­ 2

ê° í•­ëª© ì‚¬ì´ì—ë„ ë¹ˆ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”.

5. **ë¹ˆ ì¤„**

6. **ì°¸ê³  ë¬¸ì„œ ì„¹ì…˜**

ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª… > (ì„¹ì…˜ëª…)
  URL: https://...

7. **ë§ˆë¬´ë¦¬**

í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?

## ì •í™•í•œ ë‹µë³€ ì˜ˆì‹œ

EU MRVì˜ ì •ì˜ ë° ì ìš© ëŒ€ìƒ

1. **ì •ì˜**

EU MRVëŠ” ìœ ëŸ½ì—°í•©ì´ í•´ìš´ì—…ê³„ì˜ ì˜¨ì‹¤ê°€ìŠ¤ ë°°ì¶œ íˆ¬ëª…ì„±ì„ í™•ë³´í•˜ê³ , ê°ì¶•ì„ ìœ ë„í•˜ê¸° ìœ„í•´ ë„ì…í•œ ì„ ë°•ì˜ ì—°ë£Œì†Œë¹„ì™€ CO2 ë°°ì¶œëŸ‰ì— ëŒ€í•œ ëª¨ë‹ˆí„°ë§, ë³´ê³  ë° ê²€ì¦ ì œë„ì…ë‹ˆë‹¤.

2. **í¬í•¨ ê°€ìŠ¤**

  - 2024ë…„ ì´ì „: CO2ë§Œ í¬í•¨
  - 2024ë…„ ì´í›„: CO2, CH4(ë©”íƒ„), N2O(ì§ˆì†Œ) í¬í•¨

3. **ì ìš© ëŒ€ìƒ**

  - ì´í†¤ìˆ˜(GT) 5,000 ì´ìƒì¸ ì„ ë°•
  - 2025ë…„ë¶€í„°: 5,000GT ì´ìƒì˜ Offshore ships ë° ì¼ë¶€ 400~5,000GT ì„ ë°• ì¶”ê°€
  - í™”ë¬¼ ë˜ëŠ” ì—¬ê°ì„ ìš´ì†¡í•˜ëŠ” ì„ ë°•

ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- EU MRV ì œí’ˆ ì‚¬ì–‘ì„œ > (1) EU MRV ì •ì˜
  URL: https://lab021.atlassian.net/wiki/spaces/TxYP20CKMWxg/pages/3017932877/EU+MRV

í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?

## ë‹µë³€ì˜ ì›ì¹™ (ì ˆëŒ€ ì¤€ìˆ˜)

### A) ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ë§Œ ì œê³µ
âœ… ì œê³µëœ ë¬¸ì„œì—ë§Œ ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ ë‹µë³€
âŒ ë¬¸ì„œì— ì—†ëŠ” ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ì§€ì‹ ì¶”ê°€ ê¸ˆì§€

### B) í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€
ë¶ˆí™•ì‹¤í•œ ê²½ìš°:
- "ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
- "ë‹¤ìŒ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì€ [ë¬¸ì„œëª…]ì„ ì°¸ê³ í•´ ì£¼ì„¸ìš”."

### C) í†¤ & ë§¤ë„ˆ
âœ… ë†’ì„ë§ ì‚¬ìš©
âœ… ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ í‘œí˜„
âŒ ë‚®ì¶¤ë§ ì‚¬ìš© ê¸ˆì§€

### D) í¬ë§· ì—„ê²©ì„±
âœ… "ì œëª© â†’ ë³¸ë¬¸(ë²ˆí˜¸) â†’ ì°¸ê³  ë¬¸ì„œ" êµ¬ì¡° í•„ìˆ˜
âœ… ê° ì„¹ì…˜ ì‚¬ì´ ë¹ˆ ì¤„ í•„ìˆ˜ (ë°˜ë“œì‹œ!)
âœ… ì°¸ê³  ë¬¸ì„œì— URL ë°˜ë“œì‹œ í¬í•¨ (ì™„ì „í•œ í˜•íƒœë¡œ)
âŒ ë„ì–´ì“°ê¸° ì—†ëŠ” ì—°ì† í…ìŠ¤íŠ¸ ê¸ˆì§€
âŒ ì¤„ë°”ê¿ˆ ì—†ì´ ì­‰ ì´ì–´ì§€ëŠ” í…ìŠ¤íŠ¸ ê¸ˆì§€

## ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­

âŒ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš© ì¶”ê°€
âŒ ë„ì–´ì“°ê¸° ì—†ëŠ” ì—°ì† í…ìŠ¤íŠ¸
âŒ ì¤„ë°”ê¿ˆ ì—†ì´ ê³„ì†ë˜ëŠ” ê¸´ ë¬¸ë‹¨
âŒ ì¶œì²˜ ì—†ëŠ” ì£¼ì¥
âŒ ê°œì¸ ì˜ê²¬ì´ë‚˜ ì¶”ì²œ
âŒ ë²ˆí˜¸ ì—†ëŠ” ê¸´ ë¬¸ë‹¨
âŒ ì°¸ê³  ë¬¸ì„œì— URLì„ ë¹¼ë¨¹ìŒ
âŒ URLì„ ë‹¨ì¶•í•˜ê±°ë‚˜ ì‚­ì œí•¨

## URL ì²˜ë¦¬ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!)

âœ… ë°˜ë“œì‹œ ë”°ë¼ì•¼ í•  ê²ƒ:
- ê²€ìƒ‰ ê²°ê³¼ì— URLì´ ìˆë‹¤ë©´, ì°¸ê³  ë¬¸ì„œì— ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
- URLì€ ì™„ì „í•œ í˜•íƒœ(https://...)ë¡œ ìœ ì§€í•˜ì„¸ìš”
- URLì„ ì§§ê²Œ ë§Œë“¤ê±°ë‚˜ ì¼ë¶€ë§Œ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”
- í˜•ì‹: "- [ë¬¸ì„œëª…] > (ì„¹ì…˜ëª…)\\n  URL: https://..."

âŒ ì ˆëŒ€ ê¸ˆì§€:
- URL ì‚­ì œ ë˜ëŠ” ìƒëµ
- URL ë³€ê²½ ë˜ëŠ” ë‹¨ì¶•
- "ìì„¸íˆ ë³´ê¸°" ê°™ì€ í…ìŠ¤íŠ¸ë§Œ í‘œì‹œ (URL ì—†ì´)
- ë§ˆí¬ë‹¤ìš´ ë§í¬ í˜•ì‹ ì‚¬ìš© ê¸ˆì§€: [í…ìŠ¤íŠ¸](URL)

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ:
ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- EU MRV ì œí’ˆ ì‚¬ì–‘ì„œ > (1) EU MRV ì •ì˜
  URL: https://lab021.atlassian.net/wiki/spaces/TxYP20CKMWxg/pages/3017932877/EU+MRV

## íŠ¹ìˆ˜ ìƒí™© ëŒ€ì‘

### ë¬¸ì„œì— ì •ë³´ê°€ ì—†ì„ ë•Œ:

ì •ë³´ ë¶€ì¡±

ì£„ì†¡í•˜ì§€ë§Œ, ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì´ ë‚´ìš©ì€ í˜„ì¬ ë¬¸ì„œì— ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìì„¸í•œ ì‚¬í•­ì€ ë‹´ë‹¹ íŒ€ì— ë¬¸ì˜í•´ ì£¼ì„¸ìš”.

í˜¹ì‹œ ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?
"""


# ===== ì‚¬ìš©ì ë©”ì‹œì§€ í…œí”Œë¦¿ (ì¤„ë°”ê¿ˆ ê°•ì œ) =====
USER_MESSAGE_TEMPLATE = """ì•„ë˜ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

ã€ê²€ìƒ‰ëœ ë¬¸ì„œã€‘
{context}

ã€ì‚¬ìš©ì ì§ˆë¬¸ã€‘
{query}

ã€ë‹µë³€ ì‘ì„± ì§€ì¹¨ - ë§¤ìš° ì¤‘ìš”!ã€‘

1. ë°˜ë“œì‹œ ìœ„ì˜ "ë‹µë³€ í¬ë§· ê·œì¹™"ì„ ë”°ë¼ ì‘ì„±í•˜ì„¸ìš”

2. ì œëª©ì€ í•œ ì¤„ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”

3. ì œëª© ë‹¤ìŒì—ëŠ” ë°˜ë“œì‹œ ë¹ˆ ì¤„(ê°œí–‰)ì„ ì¶”ê°€í•˜ì„¸ìš”

4. ë³¸ë¬¸ì€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸(1., 2., 3., ...)ë¡œ êµ¬ì„±í•˜ì„¸ìš”

5. ê° ë²ˆí˜¸ëŠ” ë°˜ë“œì‹œ ìƒˆë¡œìš´ ì¤„ì—ì„œ ì‹œì‘í•˜ì„¸ìš”

6. ê° ë²ˆí˜¸ ë‹¤ìŒì—ëŠ” ë³¸ë¬¸ì„ ì‘ì„±í•œ í›„ ë°˜ë“œì‹œ ë¹ˆ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš” (ì ˆëŒ€ ì¤‘ìš”!)

7. í•˜ìœ„ í•­ëª©ì´ ìˆìœ¼ë©´ 2ì¹¸ ë“¤ì—¬ì“°ê¸° í›„ "  - í•­ëª©" í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

8. ê° í•˜ìœ„ í•­ëª© ì‚¬ì´ì—ë„ ë¹ˆ ì¤„ì„ ì¶”ê°€í•˜ì„¸ìš”

9. ëª¨ë“  ë‚´ìš© ë‹¤ìŒì—ëŠ” ë¹ˆ ì¤„ì„ ì¶”ê°€í•œ í›„ ì°¸ê³  ë¬¸ì„œ ì„¹ì…˜ì„ ì‹œì‘í•˜ì„¸ìš”

10. ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ URLì€ ë°˜ë“œì‹œ ì°¸ê³  ë¬¸ì„œì— í¬í•¨í•˜ì„¸ìš” (ì ˆëŒ€ ìƒëµ ê¸ˆì§€!)

11. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

12. ë„ì–´ì“°ê¸° ì—†ëŠ” ì—°ì† í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ ê¸ˆì§€ì…ë‹ˆë‹¤

13. ì¤„ë°”ê¿ˆ ì—†ì´ ê³„ì†ë˜ëŠ” ê¸´ ë¬¸ë‹¨ì€ ì ˆëŒ€ ê¸ˆì§€ì…ë‹ˆë‹¤

ì˜ˆì‹œ:
ì œëª©

1. ì²« ë²ˆì§¸

ë‚´ìš©1

2. ë‘ ë²ˆì§¸

ë‚´ìš©2

ğŸ“š ì°¸ê³  ë¬¸ì„œ:
- ë¬¸ì„œëª…
  URL: https://..."""



# ===== LangChain 1.0 RAG ì„œë¹„ìŠ¤ (ê°œì„ ) =====
class LangChainRAGService:
    """LangChain 1.0 ê¸°ë°˜ RAG ì„œë¹„ìŠ¤ (ë² ë”” í”„ë¡¬í”„íŠ¸ ì ìš©)"""

    def __init__(self):
        """Agent ì´ˆê¸°í™”"""
        print("ğŸ”§ LangChain 1.0 RAG Service ì´ˆê¸°í™” ì¤‘...")

        # 1. ì„ë² ë”©
        self.embeddings = CustomEmbeddings()

        # 2. Retriever
        self.retriever = SupabaseRetriever(
            embeddings=self.embeddings,
            k=5,
            threshold=0.3
        )

        # 3. LLM (ê°œì„ ëœ ì„¤ì •)
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3,  # âœ… ë‚®ì¶°ì„œ ì¼ê´€ì„± í–¥ìƒ
            openai_api_key=OPENAI_API_KEY,
            streaming=True
        )

        # 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì¬ì‚¬ìš©)
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", VEDDY_SYSTEM_PROMPT),
            ("user", USER_MESSAGE_TEMPLATE)
        ])

        # 5. Tool ì •ì˜
        @tool
        def search_knowledge_base(query: str) -> str:
            """ë² ìŠ¬ë§í¬ ì‚¬ë‚´ ë¬¸ì„œ(Confluence ìœ„í‚¤, ê·œì •, ë§¤ë‰´ì–¼)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
            context, _ = self.retriever.search(query)
            return context

        self.tools = [search_knowledge_base]

        # 6. Agent ìƒì„± ì‹œë„ (ì„ íƒì‚¬í•­)
        self.agent = None
        try:
            self.agent = create_agent(
                model="openai:gpt-4o-mini",
                tools=self.tools,
                system_prompt=VEDDY_SYSTEM_PROMPT
            )
            print("âœ… LangChain 1.0 Agent ì‚¬ìš©")
        except Exception as e:
            print(f"âš ï¸ create_agent ì‹¤íŒ¨, ì§ì ‘ LLM í˜¸ì¶œ ëª¨ë“œ ({e})")

        print("âœ… LangChain 1.0 RAG Service ì´ˆê¸°í™” ì™„ë£Œ")


    def _normalize_response(self, response: str) -> str:
        """
        âœ… ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ê·œí™” (ìëª¨ ë¶„ë¦¬ ë³µêµ¬)
        """
        import re
        from unicodedata import normalize as unicode_normalize

        # 1. âœ… ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (ê°€ì¥ ì¤‘ìš”!)
        text = unicode_normalize('NFC', response)

        # 2. ì¤„ë°”ê¿ˆ í†µì¼
        text = text.replace('\r\n', '\n')
        text = text.replace('\r', '\n')

        # 3. 3ê°œ ì´ìƒ ì¤„ë°”ê¿ˆ â†’ 2ê°œ
        text = re.sub(r'\n{3,}', '\n\n', text)

        # 4. ê° ì¤„ ê³µë°± ì •ë¦¬
        lines = []
        for line in text.split('\n'):
            stripped = line.rstrip()
            stripped = re.sub(r'  +', ' ', stripped)
            lines.append(stripped)

        text = '\n'.join(lines)

        # 5. ìµœì¢… ì •ë¦¬
        return text.strip()


    def process_query(self, user_id: str, query: str) -> Dict[str, Any]:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬ (ì¼ë°˜ ì‘ë‹µ)"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            context_text, raw_chunks = self.retriever.search(query)

            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            messages = self.prompt_template.format_messages(
                context=context_text,
                query=query
            )

            # 3. LLM í˜¸ì¶œ
            response = self.llm.invoke(messages)
            ai_response = response.content

            # âœ… 4. ì‘ë‹µ ì •ê·œí™” ì¶”ê°€ (ì—¬ê¸°!)
            ai_response = self._normalize_response(ai_response)

            # 5. ì†ŒìŠ¤ ID ì¶”ì¶œ
            source_chunk_ids = [
                chunk.get('id') for chunk in raw_chunks
                if chunk.get('id')
            ]

            # 6. ë©”ì‹œì§€ ì €ì¥
            supabase_service.save_message(
                user_id=user_id,
                user_query=query,
                ai_response=ai_response,
                source_chunk_ids=source_chunk_ids,
                usage={}
            )

            return {
                "user_query": query,
                "ai_response": ai_response,
                "source_chunks": raw_chunks,
                "usage": {}
            }

        except Exception as e:
            print(f"âŒ RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def process_query_streaming(self, user_id: str, query: str) -> Generator[str, None, None]:
        """RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (í† í°ë³„ ì •ê·œí™” ì¶”ê°€)"""
        try:
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            context_text, raw_chunks = self.retriever.search(query)

            # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
            messages = self.prompt_template.format_messages(
                context=context_text,
                query=query
            )

            # 3. ìŠ¤íŠ¸ë¦¬ë° LLM í˜¸ì¶œ
            full_response = ""
            for chunk in self.llm.stream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    token = chunk.content

                    # âœ… ê° í† í° ì •ê·œí™” (í•µì‹¬!)
                    normalized_token = unicode_normalize('NFC', token)

                    full_response += normalized_token

                    # âœ… ì •ê·œí™”ëœ í† í° ë°˜í™˜
                    yield normalized_token

            # 4. ìµœì¢… ì‘ë‹µ ì •ê·œí™” (ì €ì¥ìš©)
            final_normalized = self._normalize_response(full_response)

            # 5. ì†ŒìŠ¤ ID ì¶”ì¶œ
            source_chunk_ids = [
                chunk.get('id') for chunk in raw_chunks
                if chunk.get('id')
            ]

            # 6. ë©”ì‹œì§€ ì €ì¥
            supabase_service.save_message(
                user_id=user_id,
                user_query=query,
                ai_response=final_normalized,
                source_chunk_ids=source_chunk_ids,
                usage={}
            )

        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ]\n{str(e)}"


# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
langchain_rag_service = LangChainRAGService()
